---
title: Explore Factors with Entropy, Information Gain and Mutual Information
author: Morrison
date: '2023-10-13'
slug: []
categories:
  - Surveys
tags: []
draft: no
---



<pre><code>## Warning: package &#39;ggplot2&#39; was built under R version 4.3.1</code></pre>
<pre><code>## Warning: package &#39;purrr&#39; was built under R version 4.3.1</code></pre>
<pre><code>## Warning: package &#39;blogdown&#39; was built under R version 4.3.1</code></pre>
<pre><code>## Warning: package &#39;knitr&#39; was built under R version 4.3.1</code></pre>
<pre><code>## Warning: package &#39;readxl&#39; was built under R version 4.3.1</code></pre>
<pre class="r"><code># https://medium.com/@deenizzdogan/correlation-analysis-using-mutual-information-ac9fbc653029
library(infotheo)
library(DescTools)  # https://www.rdocumentation.org/packages/DescTools/versions/0.99.50/topics/Entropy</code></pre>
<pre><code>## Warning: package &#39;DescTools&#39; was built under R version 4.3.1</code></pre>
<pre><code>## 
## Attaching package: &#39;DescTools&#39;</code></pre>
<pre><code>## The following objects are masked from &#39;package:Hmisc&#39;:
## 
##     %nin%, Label, Mean, Quantile</code></pre>
<pre class="r"><code>combo &lt;- read_xlsx(path=&quot;C:/Users/rees/Documents/R/Projects/CLIENTS/COO Comp 2023/comboReportFour.xlsx&quot;)
combFull &lt;- combo[   , c(&quot;YearsRole&quot;, &quot;EqPtrs&quot;, &quot;OtherLawyers&quot;, &quot;Lawyers&quot;, &quot;Reports&quot;, &quot;Revenue&quot;, &quot;PPEP&quot;, &quot;DomOffices&quot;, &quot;BillingRate&quot;, &quot;Total&quot;, &quot;Title&quot;, &quot;City&quot;, &quot;PriorPos&quot;, &quot;EdLevel&quot;, &quot;Email&quot;)] 

# combFull &lt;- drop_na(combFull)   #117 after drop rows with NA

# entropy starts with probability of each title, prop.table, multiplied by log2 of that value, all summed
propTitle &lt;- data.frame(Percentages = prop.table(table(combFull$Title)))
# - (0.131578*log2(0.131578)) + (0.55263*log2(0.55263)) + (0.25877*log2(0.25877)) + (0.05701*log2(0.52701))  # 1.415

# entropy calculation https://stackoverflow.com/questions/27254550/calculating-entropy?rq=3
# DescTools::Entropy(table(combFull$Title))  # 1.598

library(entropy)</code></pre>
<pre><code>## 
## Attaching package: &#39;entropy&#39;</code></pre>
<pre><code>## The following objects are masked from &#39;package:infotheo&#39;:
## 
##     discretize, entropy</code></pre>
<pre class="r"><code>freqs &lt;- table(combFull$Title)/length(combFull$Title)

# info(freqs)  # 1.2315
# entropy::entropy.empirical(freqs, unit=&quot;log2&quot;) #1.598</code></pre>
<p>When a survey question collects data that is neither numeric nor text, it is probably a categorical, like Male/Female or Republican/Democrat/Independent. Once that kind of data has been collected, how can you better understand those categorical variable (aka factors) findings?</p>
<p>The <strong>entropy</strong> of a factor variable, such as COO titles, measures the amount of randomness in the distribution of the variable. Here, “distribution” simply means the string of answers the survey collects. Higher entropy indicates greater unpredictability in the factor. Stated differently, a factor’s entropy reflects the average uncertainty level or disorder in its various outcomes. Variables with higher uncertainty have higher entropy. That indicates a more heterogeneous variable with more levels and the levels have different numbers of respondents, while a lower entropy signifies a more pure and homogeneous variable (fewer levels and more balanced respondents in the levels). If all COO titles in our data were the same, the entropy would be zero. If two titles dominated and had approximately equal numbers of respondents, the entropy would still be quite low. If the four standardized titles all have a decent number of respondents, but the numbers vary widely, entropy will be higher.</p>
<p>To find the entropy of a factor, you first calculate the probability of each of its levels. With our data set of COO titles,
Administrator is 0.151,
Chief is 0.54,
Director is 0.243, and
Other is 0.0669 – the probabilities total 1.0. Then you multiply each proportion by the logarithm to base 2 of it<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a> the base 2 exponent would be -0.4728381, or 2 raised to that value.] and take the absolute value of adding those four products. The sum is
1.42, the entropy of the COO title factor.<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a></p>
<p>By comparison, the entropy of COO educational levels is 2.009; since it is higher, the educational background of COOs has more uncertainty or unpredictability. It would be harder to guess. Finally, as to the prior position of COOs, its entropy value is 2.14, which is even higher than educational level. These insights from entropy calculations can help an analyst choose which variables to include in models, such as a multiple regression model.</p>
<p>Aside from giving us a handle on the “reliability” of a variable, another common use of entropy values shows up in <strong>decision trees</strong>. A decision tree algorithm for classification creates a branch by calculating the resulting entropy (sometimes called “impurity”). From the top, the root level, a branch splits where the algorithm determines the resulting branch has the lowest entropy value. In other words, it looks for the split that maximally reduces the uncertainty in the classification labels (for exammple, the tree might be classifying the title of COOs based on other factor variables).<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a> The more entropy drops, the more information has been gained.</p>
<p>In fact, <strong>Information gain</strong> is the term for the pattern observed in data when there is a reduction in entropy. It consists of the entropy of the parent node in a decision tree minus the entropy of the child node (branch) below it. It is calculated by comparing the entropy before and after a split. The idea is to select the feature and split point that maximizes information gain. Higher information gain implies that the split provides more information about the class labels.</p>
<p>Can we improve (reduce) the entropy of the parent dataset to classify COO titles by segmenting on educational level? Information gain helps answer this question by measuring how much “information” a given level of education gives us about titles. The idea is to look at how much we can reduce the entropy of our parent node by segmenting on a given educational background.<a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a></p>
<!-- Where  Hp is the entropy of the parent (the complete, unsegmented dataset), n is the number of values of our target variable (and hence the number of child segments),  pci is the probability that an observation is in child  i (the weighting), and  Hci is the entropy of child (segment)  i. -->
<p>Turn now to a third, related measure for factors. <strong>Mutual information</strong> helps us understand how much information two factor variables share. The more the two have in common, the higher their mutual information value. It captures both linear and nonlinear relationships between the two variables unlike traditional correlation coefficients which are restricted to linear relationships.</p>
<p>Calculating the mutual information (MI) between two categorical factors, such as title and education level, can provide insights into the statistical dependence or information shared between the two variables. Mutual information measures the amount of information one variable provides about the other. It is usually measured in <strong>nats</strong>.<a href="#fn5" class="footnote-ref" id="fnref5"><sup>5</sup></a> It tells us how much information about a COO’s title we can know if we know that COO’s level of education. The magnitude of the mutual information reflects the strength of the association between the two factors. Higher MI values indicate a stronger association, while lower values suggest a weaker association.</p>
<pre class="r"><code># library(gmodels)
# contingency_table &lt;- gmodels::CrossTable(combFull$Title, combFull$EdLevel)
contingency_table &lt;- table(combFull$Title, combFull$EdLevel)
# Convert the table to a matrix and transpose it
contingency_matrix &lt;- t(as.matrix(contingency_table))
# Calculate joint probabilities
joint_probs &lt;- contingency_matrix / sum(contingency_matrix)

# Calculate entropy
entropy &lt;- -sum(joint_probs * log2(joint_probs), na.rm = TRUE)</code></pre>
<p>Mutual information is calculated by subtracting the entropy of the joint distribution of the two variables<a href="#fn6" class="footnote-ref" id="fnref6"><sup>6</sup></a> from the sum of their individual entropy values. So, if the sum of the entropy of title and the entropy of educational level is 1.65 plus 2.01, we subtract the entropy of the two variables together from the joint entropy, 3.54. That’s our (meager) mutual information!<a href="#fn7" class="footnote-ref" id="fnref7"><sup>7</sup></a></p>
<p>So, to put all the above together: the entropy of any categorical factor in our COO dataset is a value that increases the more the factor “bounces around,” the more it exhibits a haphazard pattern. If we sort each of the COOs with a given title by a second factor, such as education, we lower the entropy of title, and we can quantify that as Information Gain. Finally, with mutual information, we can quantify the likelihood of predicting a factor variable (e.g., title) if we know a second factor (e.g., educational background).</p>
<!-- End of post -->
<div class="footnotes footnotes-end-of-document">
<hr />
<ol>
<li id="fn1"><p>The log base 2 to a number is equal to the exponent value of 2 which gives the number. So, if the probability for the title of Chief is 0.54<a href="#fnref1" class="footnote-back">↩︎</a></p></li>
<li id="fn2"><p>A second calculation of entropy using a function from a different R library – DescTools – produces produces a slightly lower value, 1.65.<a href="#fnref2" class="footnote-back">↩︎</a></p></li>
<li id="fn3"><p>It is the weighted entropy, which means taking the weights of each attribute. The weights are the probability of each of the classes.<a href="#fnref3" class="footnote-back">↩︎</a></p></li>
<li id="fn4"><p>Information Gain is calculated by subtracting from the entropy of the parent factor, title, the sum of all the the ratios of the number of COOs in each educational level divided by all the COOS with a title and each of those multiplied by the entropy of educational level. Whew!<a href="#fnref4" class="footnote-back">↩︎</a></p></li>
<li id="fn5"><p>Nats are a unit of measurement for information. The term “nat” is short for “natural unit of information” and is named after the natural logarithm, which is commonly denoted as “ln” (logarithm to the base “e,” where “e” is Euler’s number, approximately 2.72). The relationship between nats (nat) and bits (bit) is that 1 nat is equal to approximately 1.4427 bits.<a href="#fnref5" class="footnote-back">↩︎</a></p></li>
<li id="fn6"><p>That values comes from a contingency table for the two variables, which leads to marginal probabilities, joint probabilities (for each cell in the table), and a final calculation of entropy.<a href="#fnref6" class="footnote-back">↩︎</a></p></li>
<li id="fn7"><p>Keep in mind that mutual information is a non-parametric measure and does not make assumptions about the form of the relationship between variables. It is particularly useful when dealing with categorical data or when you want to capture non-linear associations between variables. However, interpreting MI values requires domain knowledge and context to understand the practical significance of the associations between factors.<a href="#fnref7" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
