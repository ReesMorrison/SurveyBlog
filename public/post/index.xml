<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Savvy Surveys for Lawyers</title>
    <link>https://surveyjds.onrender.com/post/</link>
    <description>Recent content in Posts on Savvy Surveys for Lawyers</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <managingEditor>rees@reesmorrison.com (Rees Morrison)</managingEditor>
    <webMaster>rees@reesmorrison.com (Rees Morrison)</webMaster>
    <lastBuildDate>Wed, 01 Mar 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://surveyjds.onrender.com/post/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Be Cognizant of Panel Providers for Surveys</title>
      <link>https://surveyjds.onrender.com/post/2023-03-01-be-aware-of-panel-providers-for-surveys/panels/</link>
      <pubDate>Wed, 01 Mar 2023 00:00:00 +0000</pubDate>
      <author>rees@reesmorrison.com (Rees Morrison)</author>
      <guid>https://surveyjds.onrender.com/post/2023-03-01-be-aware-of-panel-providers-for-surveys/panels/</guid>
      
      <description>&lt;p&gt;In the fourth quarter of 2022, the law firm Katten Muchin Rosenman wanted to survey middle-market private equity investors.  To help the firm do so, it engaged a “leading global
third-party &lt;strong&gt;panel provider&lt;/strong&gt; platform.”   The brief methodology section in the report does not disclose the name of the panel provider or how it found 100 U.S. based respondents.  Meanwhile, what is a panel provider (aka research panel) and what do they offer survey sponsors?&lt;/p&gt;
&lt;p&gt;A panel provider maintains large rolls of people who have given information about themselves and agreed to take surveys on an ongoing basis.  Dozens of providers offer panels, including CatalystMR, Innovate, and SurveyMonkey, and the many more listed on &lt;a href=&#34;https://www.quirks.com/articles/23-top-panel-research-companies&#34;&gt;Quirks.com&lt;/a&gt;.   I do not know whether any panels are available of legal industry members.&lt;/p&gt;
&lt;p&gt;The &lt;a href=&#34;https://www.qualtrics.com/experience-management/research/research-panels-samples/&#34;&gt;Qualtrics website&lt;/a&gt;
offers five benefits of using a research panel vs recruiting respondents ad hoc:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;You can use them multiple times. Panels can take the same survey again at different intervals, giving you a way to track trends and changes with like-for-like comparisons.&lt;/li&gt;
&lt;li&gt;You develop In-depth knowledge.  You grow a deeper understanding of panel members through their continuing involvement in your research.&lt;/li&gt;
&lt;li&gt;They can be multi-purpose. A panel could also take part in other forms of research, such as focus groups and interviews.&lt;/li&gt;
&lt;li&gt;Surveying them is quicker and easier.  You save the time you would have spent in finding people to take part, collecting their contact information, and chasing down those who haven’t gotten around to taking your survey.&lt;/li&gt;
&lt;li&gt;They’re more reliable.  Panelists are likely to provide considered and accurate answers because they’re more committed and involved than people who have been recruited on a one-off basis.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Panel providers complement their &lt;strong&gt;hosting software&lt;/strong&gt; with their pre-arranged, cultivated groups of respondents.  They do more than just deliver respondents. They’ll be on hand if you want them to:&lt;/p&gt;
&lt;p&gt;•	Support you with designing your survey,&lt;/p&gt;
&lt;p&gt;•	Run the project,&lt;/p&gt;
&lt;p&gt;•	Script (program) the questions you create for the hosting software,&lt;/p&gt;
&lt;p&gt;•	Conduct computer-assisted telephone interviewing (CATI),&lt;/p&gt;
&lt;p&gt;•	Build custom panels or manage long-term private panels,&lt;/p&gt;
&lt;p&gt;•	Recruit panel members and gather firmographic profiles (what &lt;strong&gt;demographics&lt;/strong&gt; are to people, firmographics are to organizations),&lt;/p&gt;
&lt;p&gt;•	Cull and select respondents with pre-profiled targeting options or custom screening questions,&lt;/p&gt;
&lt;p&gt;•	Reach huge participant pools &amp;ndash; SurveyMonkey claims access to 80 million consumers in over 100 countries.  This vast group may be mostly irrelevant for legal industry sponsors as they rarely reach out to consumer participants.  A business-to-business panel provider would be an option for sponsors in the legal industry.&lt;/p&gt;
&lt;!-- End of post --&gt;
</description>
      
    </item>
    
    <item>
      <title>Issue a Press Release about the Survey and Report</title>
      <link>https://surveyjds.onrender.com/post/2023-03-01-issue-a-press-release-about-the-survey-and-the-report/pressrelease/</link>
      <pubDate>Wed, 01 Mar 2023 00:00:00 +0000</pubDate>
      <author>rees@reesmorrison.com (Rees Morrison)</author>
      <guid>https://surveyjds.onrender.com/post/2023-03-01-issue-a-press-release-about-the-survey-and-the-report/pressrelease/</guid>
      
      <description>&lt;p&gt;An omnipresent piece of the publicity puzzle is the &lt;strong&gt;press release&lt;/strong&gt;.  To let the world know about your survey or the report based on one, firms and vendors in the legal market issue press releases.  They send them to a selection of news outlets that might want to write about their undertaking or findings.  Reporters call about the release, ask questions and, hopefully, an editor approves their article for publication.&lt;/p&gt;
&lt;p&gt;Like a teaser but with some meat, a release is a slider rather than a quarter pounder.  The goal is to attract attention and prompt readers of the articles written about the release to download the report (and incidentally leave their all-important contact details).&lt;/p&gt;
&lt;p&gt;Successful press releases are works of art although at the same time they exhibit a formulaic structure and style.  Much has been written about crafting an effective release, one that garners media attention.  Therefore, here I will note a few points from my own experience and from the website of &lt;a href=&#34;https://mailchimp.com/resources/writing-press-releases/&#34;&gt;MailChimp&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In an ideal world, you would craft your release for a specific journalist or outlet. “Let them know if you’re interested in being featured in coverage about a wider industry trend, a feature article, or as an expert with a timely opinion.” Be sure to indicate that you know whom you’re writing to and why your survey activity is a fit for coverage in their publication.&lt;/p&gt;
&lt;p&gt;Like a newspaper article, cluster the key facts in the first paragraph or two.  Content information like in the &lt;strong&gt;introductory text&lt;/strong&gt; serves well: numbers and types of respondents, how long the series has been running, increases in participation from the previous survey, and other goodies.&lt;/p&gt;
&lt;p&gt;Quotes are stylized: “Senior executive/partner ABC is proud to blah blah.”  Positive spin whirls everywhere!&lt;/p&gt;
&lt;p&gt;Most people won’t open attachments from strangers, so limit your press release to a length and format that can be sent in the body of an email.   Link to a blog site or your own website for more elaborate background.&lt;/p&gt;
&lt;p&gt;Survey sponsors can also turn to services that will distribute the press release more broadly.  Several press release agencies are listed &lt;a href=&#34;https://www.g2.com/categories/press-release-distribution&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;One related technique, we suggest, is pre-publication distribution of the report to a favored reporter.  They will honor the embargo (blackout) because they get a head start on competitors and can scoop the others with their early piece.&lt;/p&gt;
&lt;!-- End of post --&gt;
</description>
      
    </item>
    
    <item>
      <title>Stay Alert to Try New Ideas and Resources</title>
      <link>https://surveyjds.onrender.com/post/2023-03-01-stay-alert-to-try-new-ideas-and-resources/misci/</link>
      <pubDate>Wed, 01 Mar 2023 00:00:00 +0000</pubDate>
      <author>rees@reesmorrison.com (Rees Morrison)</author>
      <guid>https://surveyjds.onrender.com/post/2023-03-01-stay-alert-to-try-new-ideas-and-resources/misci/</guid>
      
      <description>&lt;p&gt;I have collected here various items that will eventually be integrated into larger discussions. I have labeled each one with an appropriate key survey term.&lt;/p&gt;
&lt;p&gt;Conditional logic:   Rather than the &lt;strong&gt;hosting software&lt;/strong&gt; serving a question based on an earlier answer &amp;ndash; a &lt;strong&gt;conditional logic question&lt;/strong&gt;, here is an example of doing it by hand, so to speak, from the State Bar of Michigan 2023 Economics of Law Survey.  This old-fashioned method doesn&amp;rsquo;t shorten the survey and could lead to a respondent answering it by mistake. You can&amp;rsquo;t confirm that the person answered “Yes” to the earlier question.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://surveyjds.onrender.com/media/ConditionalLogicIfYesMichigan.png&#34; alt=&#34; &#34;&gt;&lt;/p&gt;
&lt;p&gt;Flyspeck your questions:  Have experienced, objective eyes pore over your questions as part of your &lt;strong&gt;stress testing&lt;/strong&gt;.   For its 2023 Lawyer Well-Being survey in early 203, the Massachusetts Bar Association explains its careful review (page 44):  “An external focus group comprised of senior in-house counsel, private practitioners, arbitrators, technical experts, representatives from arbitral institutions, academics, and third party funders provided valuable feedback on the draft questionnaire.”  Earlier it added (page 31): “[We] cognitively tested the survey before fielding it to ensure that respondents interpreted all questions appropriately.”&lt;/p&gt;
&lt;p&gt;Reach participants through other organizations:  Pinsent Masons 2022 survey on International Arbitration was “indebted to TDM/OGEMID, Global Arbitration Review and Tales of the Tribunal, as well as AAA/ICDR, ArbitralWomen, BAC/BIAC, CAM-CCBC, CAM Santiago, CIArb, CPR, CRCICA, DIS, EDAC, Energy Arbitration Club, HKIAC, ICC, JCAA, LCIA, LMAA, Mute Off Thursdays, NAI, NYIAC, SCC, SCCA SIAC and VIAC, for promoting the questionnaire amongst their members and subscribers. At least 26 organizations promoted the questionnaire!  The Association of Corporate Counsel (ACC) wrote of its 2023 CLO survey, “To further expand our reach, we also sent participation invites through other ACC partner organizations, and opportunities to participate were also sent through LinkedIn campaigns.”&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Invitation email&lt;/strong&gt;:  Further with the &lt;em&gt;Financial Times&lt;/em&gt; survey: “The survey will take only a few minutes to complete. If you would like to share the survey among patent attorneys or clients who would like to participate, you can enter their contact data at the end of this survey so they receive an invitation as well.”&lt;/p&gt;
&lt;p&gt;Email addresses (validation and professional email:  In the introduction to a survey by the Financial Times, conducted by Statista in early 2023, this paragraph struck me:&lt;/p&gt;
&lt;p&gt;“Please note that we will ask you for your professional email address at the end of the survey. To ensure high quality, we only use survey results from participants who have validated their survey results via a valid professional email address. Non-validated results will not be considered.”  The end of the questionnaire adds, “For the validation of your data we need your name and your business e-mail address. After submitting the survey, you will receive an e-mail from us with a confirmation link to validate your e-mail address.”&lt;/p&gt;
&lt;p&gt;Translated surveys:  In a survey by the &lt;em&gt;Financial Times&lt;/em&gt;, conducted by Statista during early 2023, the second page announces: “Welcome to our survey.  At this point and also during the survey, you can select your preferred language at any time. The change will take effect as soon as you continue to the next question.”  Users could shift from English to German, French, or Italian.&lt;/p&gt;
&lt;p&gt;Incentives: From a survey by the &lt;em&gt;Financial Times&lt;/em&gt;, conducted by Statista during early 2023: “In addition, for every participant completing the survey we will donate 0.50 cents to the International Committee of the Red Cross and Red Crescent (ICRC/ICRC), supporting people in crisis regions around the world.”  A survey of lawyers in 2014 by the &lt;a href=&#34;https://www.primaryresearch.com/Upload/ReportPdf/Survey%20of%20American%20Lawyers%20at%20Major%20Law%20Firms%20Tracking%20Billable%20Hours%20and%20Other%20Work%20Time.pdf&#34;&gt;Primary Research Group&lt;/a&gt; asked what respondents would like as a token of appreciation, which they referred to as “compensation”.&lt;/p&gt;
&lt;ol start=&#34;24&#34;&gt;
&lt;li&gt;How would you like to be compensated for taking this survey?&lt;/li&gt;
&lt;/ol&gt;
&lt;ol&gt;
&lt;li&gt;The results of this survey&lt;/li&gt;
&lt;li&gt;for every 5 surveys that you respond to you have the right to ask one question to
(an estimated) 100-200 lawyers at major firms in the United States and to obtain a
statistical summary of the results of that question&lt;/li&gt;
&lt;li&gt;A $50 product credit for Primary Research Group products about law firms.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Qualifier questions:  While roaming through a survey by the Financial Times, conducted by Statista during early 2023, I failed the qualifier question that appeared at the beginning.  Up popped this message: “Thank you very much, but unfortunately you do not belong to our target group!”&lt;/p&gt;
&lt;p&gt;Control who responds and one per organization: The Minority Corporate Counsel Association (MCCA) U.S. Law Firm &lt;a href=&#34;https://mcca.com/resources/2023-mcca-us-law-firm-diversity-survey/&#34;&gt;Diversity Survey&lt;/a&gt; took extra steps to obtain only one response per firm. “Submit your points of contact (POC) for the U.S. Law Firm Diversity Survey (we request only one person to submit all POCs).  Receive your unique Law Firm Diversity Survey link via e-mail from [link].”  Does this mean the MCAA created a link for each law firm?&lt;/p&gt;
&lt;p&gt;Customization and autocompletion:  A survey by the &lt;em&gt;Financial Times&lt;/em&gt;, conducted by Statista in early 2023, shows a customized sector at the top, based on what the respondent selected earlier.  The survey also features autocompletion of names of patent/IP law firms.  If a respondent starts entering the name of a firm, the &lt;strong&gt;hosting software&lt;/strong&gt; will fill in the rest.  This capability helps &lt;strong&gt;standardize&lt;/strong&gt; names of firms.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://surveyjds.onrender.com/post/2023-03-01-stay-alert-to-try-new-ideas-and-resources/MiscI_files/FinTimesPatentCustomizedandAutocomplete.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;!-- End of post --&gt;
</description>
      
    </item>
    
    <item>
      <title>Consider Alternatives to Releasing a Public Report</title>
      <link>https://surveyjds.onrender.com/post/2023-02-25-consider-alternatives-to-releasing-a-public-report/altstopublic/</link>
      <pubDate>Sat, 25 Feb 2023 00:00:00 +0000</pubDate>
      <author>rees@reesmorrison.com (Rees Morrison)</author>
      <guid>https://surveyjds.onrender.com/post/2023-02-25-consider-alternatives-to-releasing-a-public-report/altstopublic/</guid>
      
      <description>&lt;p&gt;No law firm, department, or vendor leaks its &lt;strong&gt;internal survey&lt;/strong&gt; to the outside world.  Of course not.  But here the focus instead is on external surveys.  Most sponsors take pains to prepare a handsome PDF report. Then they allow – actively encourage, in fact &amp;ndash; anyone to download it from their website, often after the downloader provides contact information. Additionally, sponsors e-mail the report to all their survey’s respondents. But those paths are not the only ones; here are seven other ways to release a survey’s findings to the public.&lt;/p&gt;
&lt;p&gt;Infographic: Every now and then a sponsor prepares an &lt;strong&gt;infographic&lt;/strong&gt; that summarizes the key discoveries from the survey.  An infographic takes time and effort to prepare, but it takes less of each than a full-scale report.  Below is an illustrative image of an infographic about e-discovery produced by Exterro’s survey in 2015.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://surveyjds.onrender.com/post/2023-02-25-consider-alternatives-to-releasing-a-public-report/AltstoPublic_files/Exterroinfographtop.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Reporter’s article:  Sometimes the sponsor shares the results of a survey with a publication, and one of its reporters writes an article, or a series of articles, around the findings.  Articles may include quotes by senior lawyers or executives of the sponsor that serve as expert commentary on the findings.&lt;/p&gt;
&lt;p&gt;Internet release:  Other sponsors spoon out their survey findings on their blog.  They might publish several pieces or one large document.  They typically include a plot or two.  I recently read about a survey where the sponsor explained that the results would be available only on its website.&lt;/p&gt;
&lt;p&gt;Thought piece:  Also known as a “white paper,” it could be the chosen vehicle for releasing figures and commenting on them extensively.  The piece is usually much longer, more text intensive, and cites related research.  It aims for deeper thought, sophisticated analyses, and more extended interpretations of the survey data.&lt;/p&gt;
&lt;p&gt;Paper version:  As I have mentioned, survey results might only be unveiled and distributed in hard copy at a conference.  Everyone has seen copies of old-fashioned, printed material left on the seats or in the packages of attendees.&lt;/p&gt;
&lt;p&gt;More methods:  Other alternatives to release findings include YouTube posts or podcasts.  These methods of distribution are unusual.  Using a self-publishing platform such as Leanpub, it would be possible to send findings to everyone who visits the platform or learns about it because they participated in the survey.  By that capability, they could get periodic updates, such as a &lt;strong&gt;cumulative report&lt;/strong&gt;.  I am not aware of anyone implementing this idea.&lt;/p&gt;
&lt;p&gt;Without doubt, an energetic sponsor might not only produce the polished PDF report but also take advantage of any of these methods to distribute results. Each has arguments for them and arguments against them.&lt;/p&gt;
&lt;!-- End of post --&gt;
</description>
      
    </item>
    
    <item>
      <title>Optimize the Return on Your Investment with Publicity</title>
      <link>https://surveyjds.onrender.com/post/2023-02-25-optimize-the-return-on-your-investment-with-publicity/publicity/</link>
      <pubDate>Sat, 25 Feb 2023 00:00:00 +0000</pubDate>
      <author>rees@reesmorrison.com (Rees Morrison)</author>
      <guid>https://surveyjds.onrender.com/post/2023-02-25-optimize-the-return-on-your-investment-with-publicity/publicity/</guid>
      
      <description>&lt;p&gt;For sponsors, reaching prospective clients motivates and dominates their thinking about whether to roll out a survey. Survey reports promote the expertise of the sponsor and raise awareness of it among potential buyers.  Burnishing a firm&amp;rsquo;s brand, a survey showcases the knowledge possessed by the partners or executives, their relationships with prospects and clients, and what they know about legal issues.  Combining those resources with proprietary data, previously unseen findings, and graphical capabilities, a survey’s published report makes tangible the smarts of the sponsor.&lt;/p&gt;
&lt;p&gt;My assessment of the best returns on investment (&lt;strong&gt;ROI&lt;/strong&gt;) for publicity is reflected in this list of potential actions. Based on my understanding of the legal industry world, I&amp;rsquo;ve put them in decreasing order of effectiveness.&lt;/p&gt;
&lt;p&gt;Articles: A reputable survey with a minimum number of respondents should generate multiple article ideas. Articles of 1200 to 2000 words usually satisfy editors, while a graphic in the article brightens their day.  You can reuse graphics that you prepared for your report. Articles can include quotes by people who are knowledgeable in the field.  A firm might team with a journalist who brings skills and an eye to sniff out new ideas, quotes, and a different perspective.&lt;/p&gt;
&lt;p&gt;Executive meetings:   Another approach is to schedule a visit to a major client or prospect.  Your entre is a tailored discussion of the findings and your recommendations, the recommendations being embedded in the report or offered extemporaneously at the meeting.  You shape your findings to fit the company’s situation and address the issues the executive(s) bring up.&lt;/p&gt;
&lt;p&gt;Breakfast briefings:  By this term I mean inviting a handful of clients, respondents, or prospects to share a bagel with a few your law firm’s partners.  During the meal they talk about the report, or anything it triggers.  The get-together lets attendees meet and network with others who have the same interest (or appetite).  While not as personalized as an executive meeting, they take less preparation, cost less per person, and yet create opportunities.&lt;/p&gt;
&lt;p&gt;Webinars:  We have already written about webinars, and how they engineer opportunities to deepen your contacts with respondents and other attendees.  Successful webinars result from thoughtful preparation, presentation skills, and technical prowess.&lt;/p&gt;
&lt;p&gt;Customized mailings:  All law firms and legal vendors have lists of key clients or prospective clients.  They might mail the report to them (and possibly collateral material).  Mailings take time and funding, but the task can be delegated and its reach is broad (plus take-up is measurable).&lt;/p&gt;
&lt;p&gt;Press releases:  Once you send a &lt;strong&gt;press release&lt;/strong&gt; to news outlets that might want to write about your findings, journalists will call about the release and want to ask questions. They are particularly eager to find out what&amp;rsquo;s new, what are the trends, and what surprised you.  The articles they write position whoever is interviewed as knowledgeable, an expert in the field. Such articles also have the benefit that the journalist might reach out to others and get their views on what you have discovered.&lt;/p&gt;
&lt;p&gt;Trade Groups:  Surveys done by law firms often focus on particular industries and their legal issues. Most industries have organized a trade group to lobby for them and to serve as a clearinghouse for information. If your survey meshes with the goals of such a group, they can help get in touch with members and let them know about the opportunity. They may well have a monthly publication or a list serve that will bring your survey, and later your report, to the attention of many potential clients.  Also, most trade groups organize conferences.&lt;/p&gt;
&lt;p&gt;Interviews:  Survey sponsors can leverage their respondents by asking to interview them.  This invitation could be extended in the &lt;strong&gt;thank you email&lt;/strong&gt; that you send with a &lt;strong&gt;canned email&lt;/strong&gt;.  What they say about the topic may stimulate articles, white papers, webinar content, and more. Law firms and providers of services to them cherish the chance to speak with executives at corporations or senior lawyers.&lt;/p&gt;
&lt;p&gt;Conferences: To put on a conference is a major undertaking, but to link the publication of a survey report to a major conference that shares a topic is an excellent idea. The conference organizers might welcome your survey invitations alerting invitees to the conference and in return they might promote your survey as a bonus for conference attendees. This kind of synergy shares similarities with what book publishers look for – you help me, I’ll help you.&lt;/p&gt;
&lt;p&gt;Blog posts: Many law firms maintain a blog, and some practice groups or individual partners host one on their own. Either way, the blog gives a means for reaching readers of the blog as well as those who come upon it by searching on the web.  Search engine optimization (SEO) techniques are beyond the scope of this guidebook.&lt;/p&gt;
&lt;p&gt;YouTube:  To be trendy and adventurous, publish a video on YouTube regarding your key findings.  It will not likely lead to much, but it is au courant and you can cite to it in your other forms of publicity.&lt;/p&gt;
&lt;p&gt;Podcasts:  This method of publicizing a survey’s findings would be unusual, but judging from the myriad legal related podcasts, many people find them rewarding to release.  For example, the law firms Baker Donelson and Buchanan Ingersoll both host podcasts.  Other podcasts that cover management issues include &amp;ldquo;The Mostly Legal Podcast&amp;rdquo;, “Legal Toolkit,” and “Un-Billable Hour.”&lt;/p&gt;
&lt;p&gt;White papers:  White papers deliver deeper thoughts than a survey report.  They have less graphic glitz and more text and they refer to other studies in the area.  A white paper should be more sophisticated, more academic, and more profound than an article.&lt;/p&gt;
&lt;p&gt;Series of surveys:  When a firm carries out a survey for a few years in a row, it builds a brand.  The &lt;strong&gt;series survey&lt;/strong&gt; becomes increasingly better known by the market and penetrates increasingly more widely.  The series can produce &lt;strong&gt;trend&lt;/strong&gt; data and analyses.&lt;/p&gt;
&lt;p&gt;Other promotional techniques exist (such as talking points for partners and marketing collateral), but the 13 techniques sketched above account for most of the survey publicity I have seen.  The intellectual capital generated by a completed survey project should propel all manner of promotions over a prolonged period.  These methods all feed on each other and boost each other.  A full-scale publicity campaign for a survey report can reap significant benefits in awareness and opportunities.&lt;/p&gt;
&lt;!-- End of post --&gt;
</description>
      
    </item>
    
    <item>
      <title>Recognize the Trade-Offs in Survey Decisions</title>
      <link>https://surveyjds.onrender.com/post/2023-02-25-recognize-the-trade-offs-in-survey-decisions/tradeoffs/</link>
      <pubDate>Sat, 25 Feb 2023 00:00:00 +0000</pubDate>
      <author>rees@reesmorrison.com (Rees Morrison)</author>
      <guid>https://surveyjds.onrender.com/post/2023-02-25-recognize-the-trade-offs-in-survey-decisions/tradeoffs/</guid>
      
      <description>&lt;p&gt;As you carry out surveys, you begin to realize that many decisions you make cut both ways, so to speak.  You gain because you take one fork, but you lose something too.  I consider these bi-valent decisions to be under the rubric of &lt;strong&gt;trade-offs&lt;/strong&gt;.  The more aware you are of what you give up to gain something, the sharper will be your survey projects.  Thought of most broadly, every survey project creates opportunity costs, a manifestation of a trade-off.  In alphabetical order below, I have compiled several other survey trade-offs I have recognized.&lt;/p&gt;
&lt;p&gt;Anonymity:  If you require respondents to say who they are or who they represent, you may undermine the candor that you seek.  You may end up with fewer surveys in your data set.  Some people are unwilling to put their name to their response and therefore bag the whole thing.  But &lt;strong&gt;anonymity&lt;/strong&gt; blocks the sponsors from a variety of useful steps.&lt;/p&gt;
&lt;p&gt;Binary questions:   Simple, yes.  Seemingly clear in their interpretation, yes.  But every “Yes/No” or “Like/Dislike” &lt;strong&gt;binary question&lt;/strong&gt; you include on your questionnaire runs the risk of forcing respondents into black-and-white choices that don’t reflect their nuanced views.&lt;/p&gt;
&lt;p&gt;Facts known to sponsor:  You don’t want to waste respondents’ time or bog them down with supplying simple facts, yet you ask them what you already can know.  You want from them answers you cannot generate yourself, not facts available to you.  The ideal &lt;strong&gt;supplemental data&lt;/strong&gt; consists of contextual information that the respondent does not know off the top of the head, such as the rating of a lawyer in a law department in terms of succession planning, the number of years a law department has retained a law firm, or the market capitalization of a publicly traded company.&lt;/p&gt;
&lt;p&gt;Length of survey:  Participation rates are in a trade-off with the perceived length of the survey as &lt;strong&gt;attrition&lt;/strong&gt; will take a toll on a long questionnaire.  The best way to shorten the time of a survey is to delete questions that add insufficient value.  Avoid the lure of letting people think, “Oh, add it as the answers might be interesting….”.&lt;/p&gt;
&lt;p&gt;Long &lt;strong&gt;introductory text&lt;/strong&gt;:  At the beginning of the questionnaire, an enticing preview of what follows will help hold casual inspectors of the survey and lessen attrition thereafter.  If people know where they are headed and the time it will take, they are more grappled.  Still, drone on an on with “here’s what’s coming” and they might never proceed on.&lt;/p&gt;
&lt;p&gt;Multiple choice vs free text:  The balance of selections versus text boxes tilts toward more text answers, as the findings will be more nuanced, even totally unexpected, but yet the labor required to sort out the comments and categorize them can be daunting.&lt;/p&gt;
&lt;p&gt;“Other” selections:  The more often respondents check “&lt;strong&gt;Other&lt;/strong&gt;”, the more work an analyst has to do with clean up.  We might have sidestepped vexation by writing, “Please select the title that most accurately corresponds to your title” and thereby eliminated the invitation to irritation (but losing detail, so a trade-off).&lt;/p&gt;
&lt;p&gt;Prioritizing questions:  They demand much more thought from a conscientious respondent than does the simpler “Check the most important” or even “Check the two most important.”  They require the person to evaluate the entire list of items relative to each other in stack order (&lt;strong&gt;ranking&lt;/strong&gt;) or against a standard scale (&lt;strong&gt;ratings&lt;/strong&gt;).  These questions are accordingly much more cognitively demanding.  As a result, the analyst can be less sure of the answers being diligent and thoughtful.  Worse, they might fatigue or frustrate respondents to the point that they drop out.  On the plus side, however, they allow more insightful findings, and thus present a trade-off.&lt;/p&gt;
&lt;p&gt;Required questions:  Sponsors of surveys often have an understandable desire to make many questions mandatory.  Respondents, however, favor few or no &lt;strong&gt;required questions&lt;/strong&gt;.  Hence the inherent trade-off: sponsors crave information, responders dislike coercion.&lt;/p&gt;
&lt;p&gt;Time periods:  Perhaps it does not matter to control the length of look back in a question: “How often have you been dissatisfied with HR?”.  You still collect a rating, and an open &lt;strong&gt;time frame&lt;/strong&gt; gives flexibility to respondents.  Or you may feel that any period designated by you is arbitrary and could err in either direction (too short or too long).  It’s a trade-off, but I strongly favor the known period (“over the past 18 months”) over a shapeless past.&lt;/p&gt;
&lt;p&gt;Rather than cataloging more instances, let’s summarize.  The more sensitive you become to trade-offs inherent in survey projects, the more you realize their ubiquity.  Every decision you make raises trade-offs, whether you are aware of them or not.  The more you are sensitive to trade-offs, they better you can weigh the various considerations.&lt;/p&gt;
&lt;!-- End of post --&gt;
</description>
      
    </item>
    
    <item>
      <title>Spot Duplicate Submissions of Individuals and Decide What to Keep</title>
      <link>https://surveyjds.onrender.com/post/2023-02-25-spot-duplicate-submissions-of-individuals-and-decide-what-to-keep/duplicates/</link>
      <pubDate>Sat, 25 Feb 2023 00:00:00 +0000</pubDate>
      <author>rees@reesmorrison.com (Rees Morrison)</author>
      <guid>https://surveyjds.onrender.com/post/2023-02-25-spot-duplicate-submissions-of-individuals-and-decide-what-to-keep/duplicates/</guid>
      
      <description>&lt;p&gt;Two responses from the same person happen when a survey is kept open for multiple months or when the sponsor sets in motion multiple ways of letting people know about the survey.  Inevitably, people forget that they have completed the survey (ouch, not a memorable survey!).   It is important to cull duplicate submissions because they will distort findings, especially if you subset your data into relatively small groups.&lt;/p&gt;
&lt;p&gt;You can cut down on the number of &lt;strong&gt;duplicate responses&lt;/strong&gt; by barring more than one response from the same ISP address.  The &lt;strong&gt;hosting software&lt;/strong&gt; may be able to assure only one submission per person (but a person who takes the survey from home and also from the office circumvents this guard).  A &lt;strong&gt;thank you e-mail&lt;/strong&gt; will also help respondents recall later that they already took the survey.&lt;/p&gt;
&lt;p&gt;The first sign of a doubleton is usually the last name of the respondent.  Last names, however, are not unique, although first and last name combinations usually do the trick. Once I joined first and last names so that my software could spot submissions by the same person.  In the old-fashioned way you can sort a name column and scan down it for repeated entries, which might be duplicates.&lt;/p&gt;
&lt;p&gt;But other clues can surface a suspect pair.  Knowing the organization of the respondent, for example, helps detect duplicate responses.  Likewise, an email address affords a way to determine if someone has entered more than one response to a survey.  Email addresses have the advantage of being unique.   Software can identify duplicates when the user tells which variable to inspect.&lt;/p&gt;
&lt;p&gt;You can deal with duplicates by keeping the latest submission, keeping the one that has the fullest set of answers, keeping the one that answered the most crucial questions, or writing to the person to ask them.  My default is to take the most recent response because that may reflect the most up-to-date data and opinions.  Time has passed and facts have changed.&lt;/p&gt;
&lt;p&gt;In a couple of instances I have looked at the responses and chosen the one that seemed the most comprehensive, in the sense of answering the most questions.  It is possible that the person turned in two surveys because they knew they had not finished the first. A variation on this solution is to combine the data of the two responses to create one response with the most answers. You don&amp;rsquo;t have to pick one or the other.&lt;/p&gt;
&lt;p&gt;if you write to somebody about their duplicate responses, their reply should guide you, but they may ask you to send them their data back, which can be a pain.  It is also possible to encourage them to take the survey a third time, and then you delete the first two responses.  I find that possibility slim.&lt;/p&gt;
&lt;p&gt;As a side note, I have been surprised at how much the two responses sometimes differ.  For a question as seemingly fixed as “How many people report to you?” you find the duplicate responses give different numbers!  Sure, the set of direct reports might have changed, but has corporate revenue in the latest fiscal year?  Also, if you ask for people to estimate percentages of time, from one answer to the next you see fluctuations.  Shifts in estimates are to be expected; shifts in fixed numbers trouble me.&lt;/p&gt;
&lt;!-- End of post --&gt;
</description>
      
    </item>
    
    <item>
      <title>Deal with Range Questions, or Don’t Ask Them </title>
      <link>https://surveyjds.onrender.com/post/2023-02-22-deal-with-range-questions-or-don-t-ask-them/ranges/</link>
      <pubDate>Wed, 22 Feb 2023 00:00:00 +0000</pubDate>
      <author>rees@reesmorrison.com (Rees Morrison)</author>
      <guid>https://surveyjds.onrender.com/post/2023-02-22-deal-with-range-questions-or-don-t-ask-them/ranges/</guid>
      
      <description>&lt;p&gt;Most survey experts look down on &lt;strong&gt;range questions&lt;/strong&gt;, those that ask for a selection from a list of ranges. “Was your total income last year: Less than $50,000; Between $50,000 and $100,000; $100,000 to $150,000” etc.  Far better to ask for specific numbers, even if they are &lt;strong&gt;approximations&lt;/strong&gt;, so that you can work with the resulting figures. You can do statistical analysis, for example, and your graphics can flourish immensely more detail.&lt;/p&gt;
&lt;p&gt;Survey sponsors still use ranges.  One justification is that personal information hides specificity when it is concealed in a range.  “I am 55-60 years old” doesn’t reveal as much as “How old are you?”   Other sponsors include range questions because respondents are unlikely to know the precise number, but they can feel comfortable picking from a range: “How many subpoenas did your company respond to in the previous 12 months?   “Less than 50; 50-200” etc.  Third, if the questionnaire collects range responses, the analyst does not have to &lt;strong&gt;binify&lt;/strong&gt; the values to create a factor.&lt;/p&gt;
&lt;p&gt;If you do use ranges, their widths should be similar.  Extending the three compensation ranges above, the highest range should not be “$450,000 to $1 million.”  Even more problematic are open-ended ranges, such as “Over 1 million.”&lt;/p&gt;
&lt;p&gt;In the Major, Lindsey &amp;amp; Africa 2022 Partner Compensation survey (conducted with Law360), the report explained at page 9 its methodology regarding ranges:&lt;/p&gt;
&lt;p&gt;“For a number of Survey questions, respondents were given ranges as response choices. For example, total compensation values were typically grouped in $50,000 ranges (e.g., $800,000 to $850,000).  In order to calculate the data for this Report, Law360 used, wherever possible, the midpoint for all responses that were expressed as ranges.&amp;quot;&lt;/p&gt;
&lt;p&gt;The midpoint of a range, like the median of a distribution, has the benefit of being easy to explain and easy to find, but it is likely to misrepresent the central tendency of the values.  Many distributions lack consistent dispersion throughout (a statistical term for the measure of  dispersion is “heteroskedasticity.”) That means the &lt;strong&gt;variance&lt;/strong&gt; is not constant throughout the sorted data. At least four tests measure heteroskedasticity: Bartlett Test; studentized Breusch Pagan Test for a linear regression model; Score Test; and the F Test.&lt;/p&gt;
&lt;p&gt;My sense of ranges is that the values thin out either as you go up or as you go down.  For example, if you ask for height of men in ranges of three inches, the results will thin from 72 to 75, as people six feet tall up to six foot three inches are much more frequent at six feet than three inches taller.  The variance of each inch will vary; heteroskedasticity will rule.  Likewise, in the United States, a range for men from 68 inches to 71 inches will likely have fewer in the 68 inches stratum than the 69- or 70-inch strata because more men are in that taller interval.  With either range, the distribution (the variance) will differ depending what height in inches you test.  What all this means is that the mid-point of a range may not represent the most likely value.&lt;/p&gt;
&lt;p&gt;Returning to the MLA report, it continues with a more puzzling analytic step, “In those cases where midpoints were not identifiable (e.g., responses where one parameter of the range was open-ended), Law360 and MLA jointly agreed on values to be used for those responses, applying consistent criteria from previous surveys.”&lt;/p&gt;
&lt;p&gt;The last step that the report explains, plugging in a value for the open-ended ranges, strikes me as quite odd, a troubling form of &lt;strong&gt;imputation&lt;/strong&gt;.   But it is not a mathematical imputation for a missing value, it is replacing the open-ended range with a subjective number “jointly agreed on”.  From what data did they pluck that number, and why didn’t they explain how they chose it?  Hand waving explanations are objectionable.&lt;/p&gt;
&lt;!-- End of post --&gt;
</description>
      
    </item>
    
    <item>
      <title>Detail the Credibility of Changes in Metrics Over Time</title>
      <link>https://surveyjds.onrender.com/post/2023-02-22-detail-the-reliability-of-changes-in-metrics-over-time/changestime/</link>
      <pubDate>Wed, 22 Feb 2023 00:00:00 +0000</pubDate>
      <author>rees@reesmorrison.com (Rees Morrison)</author>
      <guid>https://surveyjds.onrender.com/post/2023-02-22-detail-the-reliability-of-changes-in-metrics-over-time/changestime/</guid>
      
      <description>&lt;p&gt;The raison d’etre of a &lt;strong&gt;serial survey&lt;/strong&gt; is for the sponsor to discover and proclaim changes in meaningful metrics over time.  Based on the responses to their survey a year ago and the one this year, they want to shout from the rooftops that “Total legal spending as a percentage of revenue in the telecom industry climbed from 0.66 to 0.75 during the past year!”  Setting aside quibbles about &lt;strong&gt;margin of error&lt;/strong&gt;, to evaluate the legitimacy of such claims, readers should focus on the degree of similarity of the respondents last year to this year.&lt;/p&gt;
&lt;p&gt;Ideally, a serial survey ought to have a stable group of respondents answering similar questions for the two identical periods of time.  If there is churn in the participant group – last year 150 took part but this year only 100 of them did along with 60 new ones, the claim becomes more problematic.  If either the phrasing of the key question or its definitions or instructions shift – last year asked about gross revenue and this year only revenue, the purported shift becomes questionable.  If the interval moved – annual revenue one year and fiscal-year revenue the next, year-over-year comparisons become specious.&lt;/p&gt;
&lt;p&gt;It will cast even more doubt on the reliability of the asserted change in total legal spending if we consider the &lt;strong&gt;echo chamber&lt;/strong&gt; effect and the context of the question: from the &lt;strong&gt;invitation email&lt;/strong&gt;, to the title of the survey to the order of the online questionnaire.  It’s impossible to achieve ceteris paribus.  Regardless of all that, were a significant external effect to have swept over the industry or group – perhaps a recession, a massive bankruptcy, a profound shaking from a new law or court decision – that too could put into question the purported change in a key metric.&lt;/p&gt;
&lt;p&gt;A scrupulous survey report would not only highlight the change in the metric for the consistent, core group of respondents (and for the full group) but would also report honestly on any methodological challenges.&lt;/p&gt;
&lt;p&gt;Even if the community of participants has substantial similarity, such as if 90 AMLAW 200 law firms answered last year, but only 65 returned and 35 new ones took the place of the dropouts, the sponsor should not shout about shifts in a benchmark metric.  If the absolute number of participants declines, but the group in the second year stayed the same, that poses fewer or methodological problems (a form of &lt;strong&gt;response bias&lt;/strong&gt; or the &lt;strong&gt;echo chamber&lt;/strong&gt; effect might mar the data).&lt;/p&gt;
&lt;p&gt;These methodological difficulties confound all serial surveys.  The touted “continuity” of a benchmark survey may be a Potemkin village. Yes, nearly 50 years ago Equitable’s legal department commissioned Price Waterhouse to do a consulting project, from which emerged the distant ancestor of one of today’s staffing and spending surveys. Later, the survey trundled over to Hildebrandt, continued when Thomson Reuters acquired that firm, and kept plugging away when BakerRobbins merged in. Now, a survey that prides itself on its lineage perches with a fourth company.  Continuity of a survey doesn’t matter; continuity of participants and questions does, so findings of serial surveys ought to disclose turnover in the ranks.&lt;/p&gt;
&lt;p&gt;I have not found a way to capture in a single number the notion discussed here.  To say that the results from the second survey reflect answers from 62% similar respondents goes part way (100/160), but that calculation does not bring out changes in the total number of respondents in the second survey.  Nor am I clear whether to take the median of the second year carry-overs and compare it to their median of the first year, or to subtract each company’s second year metric from its first year metric and take the median of that set of numbers.&lt;/p&gt;
&lt;!-- End of post --&gt;
</description>
      
    </item>
    
    <item>
      <title>Introduce the Questionnaire at the Top</title>
      <link>https://surveyjds.onrender.com/post/2023-02-22-introduce-the-questionnaire-at-the-top/introduction/</link>
      <pubDate>Wed, 22 Feb 2023 00:00:00 +0000</pubDate>
      <author>rees@reesmorrison.com (Rees Morrison)</author>
      <guid>https://surveyjds.onrender.com/post/2023-02-22-introduce-the-questionnaire-at-the-top/introduction/</guid>
      
      <description>&lt;p&gt;At the start of your questionnaire, just below the title, the first text the respondent sees should be an &lt;strong&gt;introduction&lt;/strong&gt;.  It should explain for the newly arrived respondent both the purpose of the survey and the road map ahead. Remember, not everyone who reaches the online link for the survey will have gotten the &lt;strong&gt;invitation email&lt;/strong&gt; that explains this background information.  Someone might have forwarded the link to a group or a person without much context or detail.&lt;/p&gt;
&lt;p&gt;In the introduction you could foreshadow the number of questions, but more importantly the number of questions that require them to think, i.e., not the easy &lt;strong&gt;demographic questions&lt;/strong&gt; such as their last name.  “This survey asks a few questions about you and then 13 questions on attitudes and important numbers.”  You could describe the flow of areas of inquiry within the overall topic: “First we ask about causes of stress, then we move to possible alleviators of stress.”&lt;/p&gt;
&lt;p&gt;Too, the introduction could highlight the &lt;strong&gt;incentives&lt;/strong&gt; that thank people who complete the survey.  Few people are so altruistic that they will spend time for the advancement of knowledge or to pad the bottom line of the survey sponsor.  “What’s in it for me?” they silently ask.&lt;/p&gt;
&lt;p&gt;When you get right down to it, whatever has value in the invitation email could be reprised in the introduction.  You could stress again the measures you will take to avoid disclosure of any personal data.  You could reassure them about how long respondents typically take to complete the survey.  You might estimate when they will receive the report, and whether &lt;strong&gt;cumulative reports&lt;/strong&gt; will be released.  It might also clarify whom they should reach out to if they have a question about the survey.  Withal, the information potential respondents might want can make up both the invitation and the introductory welcome mat.&lt;/p&gt;
&lt;p&gt;In my mind, the difference is that you can format the introduction in ways that emails, which are often viewed on cell phones, might not handle.   The psychological tone also differs because the person has clicked on the link, which evidences interest, and you can pull them forward.&lt;/p&gt;
&lt;p&gt;In other words, craft the introductory text to make people as comfortable as possible about the unknowns of the survey ahead.  This is another tool to keep them going through your survey to the end; it should reduce &lt;strong&gt;attrition&lt;/strong&gt;.  That said, a &lt;strong&gt;trade-off&lt;/strong&gt; lurks: too long an introduction and a busy respondent might stop reading and call it a day.&lt;/p&gt;
&lt;p&gt;One final observation.  A well-crafted introduction might be recycled in articles about the survey report or in thumbnail descriptions of the project for journalists.&lt;/p&gt;
&lt;!-- End of post --&gt;
</description>
      
    </item>
    
  </channel>
</rss>
