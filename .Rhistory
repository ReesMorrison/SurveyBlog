unnest_tokens(word, Words) %>%
anti_join(stop_words)
sentDF <- posts %>%
get_sentences() %>%
sentiment()
sentDFHi <- sentDF %>% arrange(desc(sentiment)) %>% top_n(5)
sentDFLow <- sentDF %>% arrange(sentiment) %>% top_n(-5)
sentDFExtremes <- bind_rows(sentDFHi[  , -c(2, 3)], sentDFLow[  , -c(2, 3)])
colnames(sentDFExtremes) <- c("Sentence", "Num. Words", "Polarity Sentiment")
kable(sentDFExtremes[  , -c(2, 3)])
# extract_sentiment_terms(text.var = postSentiment$word)
kable(sentDFExtremes[  , -c(2, 3)]) %>%
row_spec(7, background = "#90ee90")
kable(sentDFExtremes[  , -c(2, 3)]) %>%
row_spec(6:10, background = "#90ee90")
LTNSentKable <- kable(sentDFExtremes[  , -c(2, 3)]) %>%
row_spec(1:5, background = "#90ee90")
kable(sentDFExtremes[  , -c(2, 3)]) %>%
row_spec(1:5, background = "#90ee90") %>%
kable_styling() %>%
save_kable(file = "C:/Users/Rees Morrison/Documents/R/Projects/LAWYER Hornbooks/5Surveys/LTNtable.png")
kable(sentDFExtremes[  , -c(2, 3)]) %>%
row_spec(1:5, background = "#90ee90") %>%
# kable_styling() %>%
save_kable(file = "C:/Users/Rees Morrison/Documents/R/Projects/LAWYER Hornbooks/5Surveys/LTNtable.png")
kable(sentDFExtremes[  , -c(2, 3)]) %>%
row_spec(1:5, background = "#90ee90")
kable(sentDFExtremes[  , -c(2, 3)]) %>%
row_spec(1:5, background = "#90ee90") %>%
# kable_styling() %>%
save_kable(file = "LTNtable.png")
library(knitr)
library(kableExtra)
library(magrittr)
kable(sentDFExtremes[  , -c(2, 3)]) %>%
row_spec(1:5, background = "#90ee90") %>%
# kable_styling() %>%
save_kable(file = "LTNtable.png")
kable(sentDFExtremes[  , -c(2, 3)]) %>%
# row_spec(1:5, background = "#90ee90") %>%
kable_styling() %>%
save_kable(file = "LTNtable.png")
help("save_kable")
kable(sentDFExtremes[  , -c(2, 3)]) %>%
row_spec(1:5, background = "#90ee90") %>%
kable_styling()
LTNKable <- kable(sentDFExtremes[  , -c(2, 3)], caption = "Five Most Positive (Green) and Negative") %>%
row_spec(1:5, background = "#90ee90") %>%
kable_styling()
save_kable(x = LTNKable, file = "LTNtable.png")
install.packages("webshot")
install.packages("webshot")
library(webshot)
save_kable(x = LTNKable, file = "LTNtable.png")
knitr::opts_chunk$set(echo = FALSE, warning=FALSE, message = FALSE)
library(tidyverse)
library(knitr)
library(bookdown)
LTNKable <- kable(sentDFExtremes[  , -c(2, 3)], caption = "Five Most Positive (Green) and Negative") %>%
row_spec(1:5, background = "#90ee90") %>%
kable_styling()
LTNKable <- kable(sentDFExtremes[  , -c(2, 3)], caption = "Five Most Positive (Green) and Negative") %>%
row_spec(1:5, background = "#90ee90") %>%
kableExtra::kable_styling()
library(kableExtra)
LTNKable <- kable(sentDFExtremes[  , -c(2, 3)], caption = "Five Most Positive (Green) and Negative") %>%
row_spec(1:5, background = "#90ee90") %>%
kableExtra::kable_styling()
save_kable(x = LTNKable, file = "LTNtable.png")
kableExtra::save_kable(x = LTNKable, file = "LTNtable.png")
traceback()
kable(sentDFExtremes[  , -c(2, 3)], caption = "Five Most Positive (Green) and Negative Text Comments") %>%
row_spec(1:5, background = "#90ee90") %>%
kableExtra::kable_styling()
library(tm)
library(tidytext)
library(tidytext)
postAssoc <- posts %>%            # at 7:02
unnest_tokens(output = word, input = Words) %>%
anti_join(stop_words, by = "word") %>%
filter(str_detect(word, "[:alpha:]")) %>%
distinct()
View(postAssoc)
View(posts)
postAssoc <- posts[-c(1:11), ] %>%            # at 7:02; 2,840 obs
unnest_tokens(output = word, input = Words) %>%
anti_join(stop_words, by = "word") %>%
filter(str_detect(word, "[:alpha:]")) %>%
distinct()
postAssoc <- posts %>%            # at 7:02; 2,840 obs
unnest_tokens(output = word, input = Words) %>%
anti_join(stop_words, by = "word") %>%
filter(str_detect(word, "[:alpha:]")) %>%
distinct()
mentions <- postAssoc %>%
count(word, name = "users_n") %>%
filter(users_n <= 10)
mentions <- postAssoc %>%
count(word, name = "users_n") %>%
filter(users_n >= 10)
mentions <- postAssoc %>%
count(word, name = "users_n") %>%
filter(users_n >= 5)
mentions <- postAssoc %>%
count(word, name = "users_n") %>%
filter(users_n >= 2)
mentions <- postAssoc %>%
count(word, name = "users_n")
View(mentions)
library(textstem)
postAssoc$lemma <- lemmatize_words(postAssoc$word)
mentions <- postAssoc %>%
count(lemma, name = "users_n") %>%
filter(users_n >= 2)
mentions <- postAssoc %>%
count(lemma, name = "users_n") %>%
filter(users_n >= 3)
View(mentions)
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(readxl)
library(ggthemes)
input <- read_lines("C:/Users/Rees Morrison/Documents/R/Projects/LAWYER Hornbooks/5Surveys/SurveyBook/SavSurveyGuide.Rmd")
one <- str_extract_all(string = input, pattern = "^\\# .+\\#?", simplify = TRUE)
View(one)
rm(one)
blogposts <- str_extract_all(string = input, pattern = "^\\#\\# .+\\#\\#?", simplify = TRUE)
View(blogposts)
head(input)
head(input, 100)
str_extract_all(string = "## Choose a Complementary Team for the Survey Project blah blah ## " , pattern = "^\\#\\# .+\\#\\#?", simplify = TRUE)
str_extract_all(string = "## Choose a Complementary Team for the Survey Project blah blah ## second post ## Choose a Complementary Team for the Survey Project" " , pattern = "^\\#\\# .+\\#\\#?", simplify = TRUE)
str_extract_all(string = "## Choose a Complementary Team for the Survey Project blah blah ## second post ## Choose a Complementary Team for the Survey Project", pattern = "^\\#\\# .+\\#\\#?", simplify = TRUE)
blogposts <- str_extract_all(string = input, pattern = "^\\#\\# .+\\#\\#?", simplify = TRUE)
View(blogposts)
blogposts <- str_extract_all(string = input[1:500], pattern = "^\\#\\# .+\\#\\#?", simplify = TRUE)
input[1:500]
blogposts <- str_extract_all(string = text, pattern = "^\\#{2} .+\\#{2}?", simplify = TRUE)
input <- read_lines("C:/Users/Rees Morrison/Documents/R/Projects/LAWYER Hornbooks/5Surveys/SurveyBook/SavSurveyGuide.Rmd")
# combine all the text, with a space between each term
text <- paste0(input, collapse = " ")
blogposts <- str_extract_all(string = text, pattern = "^\\#{2} .+\\#{2}?", simplify = TRUE)
blogposts <- str_extract(string = text, pattern = "^\\#\\# .+\\#{2}?", simplify = TRUE)
blogposts <- str_extract(string = text, pattern = "^\\#\\# .+\\#{2}?")
blogposts <- str_extract(string = text, pattern = "\\#\\# .+\\#{2}?")
blogposts
blogposts <- str_extract(string = text, pattern = "\\#\\# .+\\#{2} ?")
head(blogposts)
blogposts <- str_extract(string = text, pattern = "\\#\\#\\s.*\\#{2}?")
head(blogposts, 1)
head(text, 1)
blogposts <- str_extract(string = text, pattern = "^\\#\\#\\s.*\\#{2}?")
blogposts <- str_extract(string = "## Test blah blah ## second", pattern = "^\\#\\#\\s.*\\#{2}?")
str_extract(string = "garbage ## Test blah blah ## second", pattern = "^\\#\\#\\s.*\\#{2}?")
str_extract(string = "garbage ## Test blah blah ## second", pattern = "\\#\\#\\s.*\\#{2}?")
str_extract(string = "garbage ## Test blah blah ## second garbage ## Test blah blah ## second", pattern = "\\#\\#\\s.*\\#{2}?")
blogposts <- str_extract(string = text, pattern = "\\#\\#\\s.*?\\#{2}")
str_extract(string = "garbage ## Test blah blah ## second garbage ## Test blah blah ## second", pattern = "\\#\\#\\s.*?\\#{2}")
blogposts <- str_extract_all(string = text, pattern = "\\#\\#\\s.*?\\#{2}")
blogposts <- str_extract_all(string = text, pattern = "\\#\\#\\s.*?\\#{2}", simplify = TRUE)
View(blogposts)
blogposts$V5
blogposts[1, 5]
text <- str_replace_all(text, "\\#\\#\\#", replacement = "zzz")
blogposts <- str_extract_all(string = text, pattern = "\\#\\#\\s.*?\\#{2}", simplify = TRUE)
View(blogposts)
blogposts <- str_extract_all(string = text, pattern = "\\#\\#\\s.*?<!-- End of post -->", simplify = TRUE)
blogposts <- str_extract_all(string = text, pattern = "\\#\\#\\s.*?End of post", simplify = TRUE)
input <- read_lines("C:/Users/Rees Morrison/Documents/R/Projects/LAWYER Hornbooks/5Surveys/SurveyBook/SavSurveyGuide.Rmd")
# combine all the text, with a space between each term
text <- paste0(input, collapse = " ")
text <- str_replace_all(text, "\\#\\#\\#", replacement = "zzz")
blogposts <- str_extract_all(string = text, pattern = "\\#\\#\\s.*?End of post", simplify = TRUE)
View(blogposts)
blogposts <- t(str_extract_all(string = text, pattern = "\\#\\#\\s.*?End of post", simplify = TRUE))
View(blogposts)
blogposts[1, 1]
blogposts <- t(str_extract_all(string = text, pattern = "\\#\\#\\s.*?<!-- End of post", simplify = TRUE))
blogposts[1, 1]
blogposts <- t(str_extract_all(string = text, pattern = "\\#\\#\\s.*?\\<\\!-- End of post", simplify = TRUE))
blogposts[1, 1]
blogposts[ , 1] <- str_remove_all(blogposts[ , 1], "<!-- End of post")
blogposts <- t(str_extract_all(string = text, pattern = "\\#\\#\\s.*?End of post", simplify = TRUE))
blogposts[ , 1] <- str_remove_all(blogposts[ , 1], "<!-- End of post")
blogposts[1, 1]
blogposts$terms <- blogposts[  , 1]
text <- str_replace_all(text, "\\#\\#\\#", replacement = "zzz")
blogposts <- t(str_extract_all(string = text, pattern = "\\#\\#\\s.*?End of post", simplify = TRUE))
blogposts[ , 1] <- str_remove_all(blogposts[ , 1], "<!-- End of post")
colnames(blogposts) <- "PostName"
blogposts$PostName <- str_remove_all(blogposts$PostName, "<!-- End of post")
View(blogposts)
blogposts <- data.frame(t(str_extract_all(string = text, pattern = "\\#\\#\\s.*?End of post", simplify = TRUE)))
colnames(blogposts) <- "PostName"
blogposts$PostName <- str_remove_all(string = blogposts$PostName, pattern = "<!-- End of post")
blogposts$Terms <- blogposts$PostName
blogposts <- blogposts %>%
unnest_tokens(word, Terms) %>%
anti_join(stop_words)
View(blogposts)
input <- read_lines("C:/Users/Rees Morrison/Documents/R/Projects/LAWYER Hornbooks/5Surveys/SurveyBook/SavSurveyGuide.Rmd")
# combine all the text, with a space between each term
text <- paste0(input, collapse = " ")
text <- str_replace_all(text, "\\#\\#\\#", replacement = "zzz")
blogposts <- data.frame(t(str_extract_all(string = text, pattern = "\\#\\#\\s.*?End of post", simplify = TRUE)))
colnames(blogposts) <- "PostName"
blogposts$PostName <- str_remove_all(string = blogposts$PostName, pattern = "<!-- End of post")
blogposts$Terms <- blogposts$PostName  # duplicate the orginal column, so I can remove all but the name of the post in the first column
blogposts$PostName <- str_remove_all(string = blogposts$PostName, pattern = "\\:.*$")
View(blogposts)
blogposts$PostName <- str_remove_all(string = blogposts$PostName, pattern = "\\#\\#")
View(blogposts)
# tokenize each post
blogposts <- blogposts %>%
unnest_tokens(word, Terms) %>%
anti_join(stop_words)
blogposts$Lemmas <- lemmatize_words(blogposts$word)
blogposts <- blogposts %>%
unnest_tokens(word, Terms) %>%
anti_join(stop_words) %>%
filter(str_detect(word, "[:alpha:]")) %>%
distinct()
# tokenize each post  with 26 posts, have 8660
blogposts <- blogposts %>%
unnest_tokens(word, Terms) %>%
anti_join(stop_words) %>%
filter(str_detect(word, "[:alpha:]"))
input <- read_lines("C:/Users/Rees Morrison/Documents/R/Projects/LAWYER Hornbooks/5Surveys/SurveyBook/SavSurveyGuide.Rmd")
# combine all the text, with a space between each term
text <- paste0(input, collapse = " ")
#Replace ### with ZZ so that I don't extract them.
text <- str_replace_all(text, "\\#\\#\\#", replacement = "zzz")
blogposts <- data.frame(t(str_extract_all(string = text, pattern = "\\#\\#\\s.*?End of post", simplify = TRUE)))
colnames(blogposts) <- "PostName"
blogposts$Terms <- blogposts$PostName  # dupe column to remove all but name of post in it
blogposts$PostName <- str_remove_all(string = blogposts$PostName, pattern = "\\:.*$")
blogposts$PostName <- str_remove_all(string = blogposts$PostName, pattern = "\\#\\#")
View(blogposts)
# tokenize each post  with 26 posts, have 8660
blogposts <- blogposts %>%
unnest_tokens(word, Terms) %>%
anti_join(stop_words, by = "word") %>%
filter(str_detect(word, "[:alpha:]"))
blogposts$Lemmas <- lemmatize_words(blogposts$word)
mentions <- blogposts %>%
count(Lemmas, name = "users_n") %>%
filter(users_n >= 3)  # at >=2 = 503; this is 116
View(mentions)
library(widyr)
install.packages("widyr")
posts_who_mention_word <- blogposts %>%
count(Lemmas, name = "users_n") %>%
filter(users_n >= 3)  # at >=2 = 503; this is 116
library(widyr)
word_correlations <- blogposts %>%     # at 8:13 of YouTube
semi_join(mentions, by = "word") %>%
pairwise_cor(item = word, feature = user_name) %>%
filter(correlation >= 0.2)
word_correlations <- blogposts %>%     # at 8:13 of YouTube
semi_join(posts_who_mention_word, by = "word") %>%
pairwise_cor(item = word, feature = user_name) %>%
filter(correlation >= 0.2)
word_correlations <- blogposts %>%     # at 8:13 of YouTube
semi_join(posts_who_mention_word, by = c("word" = "Lemmas"))
word_correlations <- blogposts %>%     # at 8:13 of YouTube
semi_join(posts_who_mention_word, by = c("word" = "Lemmas")) %>%
pairwise_cor(item = word, feature = user_name) %>%
filter(correlation >= 0.2)
word_correlations <- blogposts %>%     # at 8:13 of YouTube
semi_join(posts_who_mention_word, by = c("word" = "Lemmas")) %>%
pairwise_cor(item = word, feature = PostName) %>%
filter(correlation >= 0.2)
posts_who_mention_word <- blogposts %>%
count(Lemmas, name = "users_n") %>%
filter(users_n >= 4)  # at >=3 = 705 obs
word_correlations <- blogposts %>%     # at 8:13 of YouTube
semi_join(posts_who_mention_word, by = c("word" = "Lemmas")) %>%
pairwise_cor(item = word, feature = PostName) %>%
filter(correlation >= 0.3)
library(igraph)
library(ggraph)
graph_from_data_frame(d = word_correlations,   # the edge list, at 9:54 of Youtube
vertices = posts_who_mention_word) %>%
ggraph(layout = "fr") +
geom_edge_link() +
geom_node_point() +
geom_node_text(aes(label = name))
posts_who_mention_word <- blogposts %>%
count(Lemmas, name = "users_n") %>%
filter(users_n >= 5)
word_correlations <- blogposts %>%     # at 8:13 of YouTube
semi_join(posts_who_mention_word, by = c("word" = "Lemmas")) %>%
pairwise_cor(item = word, feature = PostName) %>%
filter(correlation >= 0.4)
graph_from_data_frame(d = word_correlations,   # the edge list, at 9:54 of Youtube
vertices = posts_who_mention_word) %>%
ggraph(layout = "fr") +
geom_edge_link() +
geom_node_point() +
geom_node_text(aes(label = name))  # at 11:32 on YouTube
posts_who_mention_word <- blogposts %>%
count(Lemmas, name = "users_n") %>%
filter(users_n >= 6)
word_correlations <- blogposts %>%     # at 8:13 of YouTube
semi_join(posts_who_mention_word, by = c("word" = "Lemmas")) %>%
pairwise_cor(item = word, feature = PostName) %>%
filter(correlation >= 0.5)
graph_from_data_frame(d = word_correlations,   # the edge list, at 9:54 of Youtube
vertices = posts_who_mention_word) %>%
ggraph(layout = "fr") +
geom_edge_link() +
geom_node_point() +
geom_node_text(aes(label = name))  # at 11:32 on YouTube
graph_from_data_frame(d = word_correlations,   # the edge list, at 9:54 of Youtube
vertices = posts_who_mention_word %>%
#remove words that appear only once at 13:02
semi_join(word_correlations, by = c("word" = "item1"))) %>%
ggraph(layout = "fr") +
geom_edge_link() +
geom_node_point() +
geom_node_text(aes(label = name))  # at 11:32 on YouTube
graph_from_data_frame(d = word_correlations,   # the edge list, at 9:54 of Youtube
vertices = posts_who_mention_word %>%
#remove words that appear only once at 13:02
semi_join(word_correlations, by = c("Lemmas" = "item1"))) %>%
ggraph(layout = "fr") +
geom_edge_link() +
geom_node_point() +
geom_node_text(aes(label = name))  # at 11:32 on YouTube
posts_who_mention_word <- blogposts %>%
count(Lemmas, name = "users_n") %>%
filter(users_n >= 8)
word_correlations <- blogposts %>%     # at 8:13 of YouTube
semi_join(posts_who_mention_word, by = c("word" = "Lemmas")) %>%
pairwise_cor(item = word, feature = PostName) %>%
filter(correlation >= 0.5)
graph_from_data_frame(d = word_correlations,   # the edge list, at 9:54 of Youtube
vertices = posts_who_mention_word %>%
#remove words that appear only once at 13:02
semi_join(word_correlations, by = c("Lemmas" = "item1"))) %>%
ggraph(layout = "fr") +
geom_edge_link() +
geom_node_point() +
geom_node_text(aes(label = name))  # at 11:32 on YouTube
# from YouTube https://www.youtube.com/watch?v=ae_XVhjHd_o  code below at 7:24
posts_who_mention_word <- blogposts %>%
count(Lemmas, name = "users_n") %>%
filter(users_n >= 9)  # at 3 = 705; at 4 = 519; at 5 = 391; at 6 310; 8 = 213
word_correlations <- blogposts %>%     # at 8:13 of YouTube
semi_join(posts_who_mention_word, by = c("word" = "Lemmas")) %>%
pairwise_cor(item = word, feature = PostName) %>%
filter(correlation >= 0.6)  #27,652 at 0.3; at 0.4 8764 obs; at 0.5 and 8 post = 2214
graph_from_data_frame(d = word_correlations,   # the edge list, at 9:54 of Youtube
vertices = posts_who_mention_word %>%
#remove words that appear only once at 13:02
semi_join(word_correlations, by = c("Lemmas" = "item1"))) %>%
ggraph(layout = "fr") +
geom_edge_link() +
geom_node_point() +
geom_node_text(aes(label = name), repel = TRUE)  # at 11:32 on YouTube; up to 13:37 repel
ggsave(filename = "LTNWordAssoc.png", path = "C:/Users/Rees Morrison/Documents/R/Projects/LAWYER Hornbooks/5Surveys/", dpi = 300)
graph_from_data_frame(d = word_correlations,   # the edge list, at 9:54 of Youtube
vertices = posts_who_mention_word %>%
#remove words that appear only once at 13:02
semi_join(word_correlations, by = c("Lemmas" = "item1"))) %>%
ggraph(layout = "fr") +
geom_edge_link(aes(alpha = correlation)) +  # darker the edge, higher the corr
geom_node_point() +
geom_node_text(aes(color = users_n, label = name), repel = TRUE)  # at 11:32 on YouTube; up to 13:37 repel; lighter blue the text, the more posts contain the word
ggsave(filename = "LTNWordAssoc.png", path = "C:/Users/Rees Morrison/Documents/R/Projects/LAWYER Hornbooks/5Surveys/", dpi = 300)
knitr::opts_chunk$set(echo = FALSE, warning=FALSE, message = FALSE)
library(tidyverse)
library(knitr)
library(bookdown)
library(tidytext)
knitr::opts_chunk$set(echo = FALSE, warning=FALSE, message = FALSE)
library(knitr)
figPath <- "C:/Users/Rees Morrison/Documents/R/Projects/CLIENTS/Themes/BlogThemes1/static/media/"
library(tidyverse)  # frequently used packages that follow a similar coding approach
library(blogdown)
library(knitr)
library(funModeling) # to compare vectors against each other, such as URLs of posts
library(ggthemes) # to include the theme_tufte them in plots
library(lubridate) # to deal with date data, such as the date a post is published
library(ggrepel) # to spread points or text so that they do not overlap on a plot
library(readxl) # to read in the underlying Excel data
library(tidytext) # text-mining functions that help analyse the content of posts
library(writexl)
library(kableExtra)
serve_site()
blogdown:::new_post_addin()
wd()
getwd()
blogdown:::insert_image_addin()
blogdown:::insert_image_addin()
blogdown:::insert_image_addin()
blogdown:::new_post_addin()
blogdown:::insert_image_addin()
blogdown:::insert_image_addin()
stop_server()
knitr::opts_chunk$set(echo = FALSE, warning=FALSE, message = FALSE)
library(knitr)
figPath <- "C:/Users/Rees Morrison/Documents/R/Projects/CLIENTS/Themes/BlogThemes1/static/media/"
library(tidyverse)  # frequently used packages that follow a similar coding approach
library(blogdown)
library(knitr)
library(funModeling) # to compare vectors against each other, such as URLs of posts
library(ggthemes) # to include the theme_tufte them in plots
library(lubridate) # to deal with date data, such as the date a post is published
library(ggrepel) # to spread points or text so that they do not overlap on a plot
library(readxl) # to read in the underlying Excel data
library(tidytext) # text-mining functions that help analyse the content of posts
library(writexl)
library(kableExtra)
serve_site()
blogdown:::new_post_addin()
blogdown:::new_post_addin()
stop_server()
SurvAskEmail <- read.table("~/R/Projects/LAWYER Hornbooks/5Surveys/SurveyBlog/content/post/2022-05-03-surveysaskemailaddresses/SurvAskEmail.Rmd", header=TRUE, quote="\"")
View(SurvAskEmail)
knitr::opts_chunk$set(echo = FALSE, warning=FALSE, message = FALSE)
library(knitr)
figPath <- "C:/Users/Rees Morrison/Documents/R/Projects/CLIENTS/Themes/BlogThemes1/static/media/"
library(tidyverse)  # frequently used packages that follow a similar coding approach
library(blogdown)
library(knitr)
library(funModeling) # to compare vectors against each other, such as URLs of posts
library(ggthemes) # to include the theme_tufte them in plots
library(lubridate) # to deal with date data, such as the date a post is published
library(ggrepel) # to spread points or text so that they do not overlap on a plot
library(readxl) # to read in the underlying Excel data
library(tidytext) # text-mining functions that help analyse the content of posts
library(writexl)
library(kableExtra)
serve_site()
blogdown:::new_post_addin()
stop_server()
knitr::opts_chunk$set(echo = FALSE, warning=FALSE, message = FALSE)
library(knitr)
figPath <- "C:/Users/Rees Morrison/Documents/R/Projects/CLIENTS/Themes/BlogThemes1/static/media/"
library(tidyverse)  # frequently used packages that follow a similar coding approach
library(blogdown)
library(knitr)
library(funModeling) # to compare vectors against each other, such as URLs of posts
library(ggthemes) # to include the theme_tufte them in plots
library(lubridate) # to deal with date data, such as the date a post is published
library(ggrepel) # to spread points or text so that they do not overlap on a plot
library(readxl) # to read in the underlying Excel data
library(tidytext) # text-mining functions that help analyse the content of posts
library(writexl)
library(kableExtra)
serve_site()
blogdown:::new_post_addin()
blogdown:::new_post_addin()
stop_server()
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(readxl)
library(ggthemes)
library(reticulate)
library(pluralize)  # to lemmatize
knitr::opts_chunk$set(echo = FALSE, warning=FALSE, message = FALSE)
library(knitr)
figPath <- "C:/Users/Rees Morrison/Documents/R/Projects/CLIENTS/Themes/BlogThemes1/static/media/"
library(tidyverse)  # frequently used packages that follow a similar coding approach
library(blogdown)
library(knitr)
library(funModeling) # to compare vectors against each other, such as URLs of posts
library(ggthemes) # to include the theme_tufte them in plots
library(lubridate) # to deal with date data, such as the date a post is published
library(ggrepel) # to spread points or text so that they do not overlap on a plot
library(readxl) # to read in the underlying Excel data
library(tidytext) # text-mining functions that help analyse the content of posts
library(writexl)
library(kableExtra)
serve_site()
blogdown:::new_post_addin()
knitr::opts_chunk$set(echo = FALSE, warning=FALSE, message = FALSE)
library(knitr)
figPath <- "C:/Users/Rees Morrison/Documents/R/Projects/CLIENTS/Themes/BlogThemes1/static/media/"
library(tidyverse)  # frequently used packages that follow a similar coding approach
library(blogdown)
library(knitr)
library(funModeling) # to compare vectors against each other, such as URLs of posts
library(ggthemes) # to include the theme_tufte them in plots
library(lubridate) # to deal with date data, such as the date a post is published
library(ggrepel) # to spread points or text so that they do not overlap on a plot
library(readxl) # to read in the underlying Excel data
library(tidytext) # text-mining functions that help analyse the content of posts
library(writexl)
library(kableExtra)
serve_site()
blogdown:::new_post_addin()
stop_server()
serve_site()
stop_server()
knitr::opts_chunk$set(echo = FALSE, warning=FALSE, message = FALSE)
library(knitr)
figPath <- "C:/Users/Rees Morrison/Documents/R/Projects/CLIENTS/Themes/BlogThemes1/static/media/"
library(tidyverse)  # frequently used packages that follow a similar coding approach
library(blogdown)
library(knitr)
library(funModeling) # to compare vectors against each other, such as URLs of posts
library(ggthemes) # to include the theme_tufte them in plots
library(lubridate) # to deal with date data, such as the date a post is published
library(ggrepel) # to spread points or text so that they do not overlap on a plot
library(readxl) # to read in the underlying Excel data
library(tidytext) # text-mining functions that help analyse the content of posts
library(writexl)
library(kableExtra)
serve_site()
stop_server()
