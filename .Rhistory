library(tidyverse)  # frequently used packages that follow a similar coding approach
library(blogdown)
library(knitr)
library(funModeling) # to compare vectors against each other, such as URLs of posts
library(ggthemes) # to include the theme_tufte them in plots
library(lubridate) # to deal with date data, such as the date a post is published
library(ggrepel) # to spread points or text so that they do not overlap on a plot
library(readxl) # to read in the underlying Excel data
library(tidytext) # text-mining functions that help analyse the content of posts
library(writexl)
library(kableExtra)
library(tidyverse)  # frequently used packages that follow a similar coding approach
library(blogdown)
library(knitr)
library(funModeling) # to compare vectors against each other, such as URLs of posts
library(ggthemes) # to include the theme_tufte them in plots
library(lubridate) # to deal with date data, such as the date a post is published
library(ggrepel) # to spread points or text so that they do not overlap on a plot
library(readxl) # to read in the underlying Excel data
library(tidytext) # text-mining functions that help analyse the content of posts
library(writexl)
library(kableExtra)
serve_site()
postWords <- posts %>%
unnest_tokens(word, Words) %>%
anti_join(stop_words) %>%
count(word, sort = TRUE) %>%
filter(n > 40) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n)) +
geom_col() +
labs(x = NULL, y = NULL, title = "Words in Blog Posts that Appear the Most", subtitle = "From Savvy Surveys for Lawyers, frequency greater than 40") +
coord_flip() +
theme_tufte()
posts <- read_lines("C:/Users/Rees Morrison/Documents/R/Projects/LAWYER Hornbooks/5Surveys/SurveyBook/SavSurveyGuide.Rmd")
# remove lines until start of blog posts and then this block of code for article 836 obs
posts <- data.frame(Words = posts[-c(1:71, 808:900)])
posts$Words[58]
# remove ### lines (but they will disappear) and R code chunks for inserting graphic
# posts$Words <- str_remove_all(string = posts$Words, pattern = "^\\*{3}.+$")
posts$Words <- str_remove_all(string = posts$Words, pattern = "^include_.+$")
posts$Words <- str_remove_all(string = posts$Words, pattern = "^\\{r.+$")
library(gghighlight)
# gghighlight package adds a gghighlight() layer to an existing plot with conditions for filtering. All data points that do not fulfill these conditions are greyed out.
library(ggthemes)
library(ggrepel)
library(RColorBrewer)
postWords <- posts %>%
unnest_tokens(word, Words) %>%
anti_join(stop_words) %>%
count(word, sort = TRUE) %>%
filter(n > 40) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n)) +
geom_col() +
labs(x = NULL, y = NULL, title = "Words in Blog Posts that Appear the Most", subtitle = "From Savvy Surveys for Lawyers, frequency greater than 40") +
coord_flip() +
theme_tufte()
postWords
ggsave(filename = "LTNBarplot.png", path = "C:/Users/Rees Morrison/Documents/R/Projects/LAWYER Hornbooks/5Surveys/")
ggsave(filename = "LTNBarplot.png", path = "C:/Users/Rees Morrison/Documents/R/Projects/LAWYER Hornbooks/5Surveys/", dpi = 300)
library(ggwordcloud)
pal <- brewer.pal(6, "Dark2")
postCloud <- posts %>%
unnest_tokens(word, Words) %>%
anti_join(stop_words) %>%
count(word, sort = TRUE) %>%
# filter(n > 40) %>%
mutate(word = reorder(word, n))
ggwordcloud(words = postCloud$word, freq = postCloud$n, colors = pal, max.words = 75)
ggsave(filename = "LTNWordCloud.png", path = "C:/Users/Rees Morrison/Documents/R/Projects/LAWYER Hornbooks/5Surveys/", dpi = 300)
NLPBiGram <- posts %>%    #21529 obs,
unnest_tokens(bigram, Words, token = "ngrams", n = 2) %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word) %>%
mutate(stem1 = word1, stem2 = word2)
# when I stemmed before creating bigrams, all the words were put in alphabetical order!
# bigram counts   2,288 obs
bigram_counts <- NLPBiGram %>%
count(word1, word2, sort = TRUE)
bigram_counts <- bigram_counts[-1, ]  # removes na
bigram_graph <- bigram_counts %>%
filter(n > 6) %>%
igraph::graph_from_data_frame()
set.seed(2022)
library(ggraph)
ggraph(bigram_graph, layout = "fr") +
geom_edge_link() +
geom_node_point() +
geom_node_text(aes(label = name), vjust = 1, hjust = 1, size = 3) +
labs(title = "Network Graph of Bigrams in Posts of Savvy Surveys for Lawyers", subtitle = "Bigrams occuring more than six times")
ggsave(filename = "LTNBigram.png", path = "C:/Users/Rees Morrison/Documents/R/Projects/LAWYER Hornbooks/5Surveys/", dpi = 300)
postCloud %>%
ggplot(aes(label = word, size = n)) +
geom_text_wordcloud(eccentricity = 1) +
scale_size_area(max_size = 10)+
theme_minimal()
postCloud %>%
filter(n > 5) %>%
ggplot(aes(label = word, size = n)) +
geom_text_wordcloud(eccentricity = 1) +
scale_size_area(max_size = 10)+
theme_minimal()
postCloud %>%
filter(n > 15) %>%
ggplot(aes(label = word, size = n)) +
geom_text_wordcloud(eccentricity = 1) +
scale_size_area(max_size = 10)+
theme_minimal()
postCloud %>%
filter(n > 15) %>%
ggplot(aes(label = word, size = n, col = as.character(n))) +
geom_text_wordcloud(rm_outside = TRUE, max_steps = 1,
grid_size = 1, eccentricity = .9)+
scale_size_area(max_size = 14)+
scale_color_brewer(palette = "Paired", direction = -1)+
theme_void()
postCloud %>%
filter(n > 15) %>%
ggplot(aes(label = word, size = n, col = as.character(n))) +
geom_text_wordcloud(rm_outside = TRUE, max_steps = 1,
grid_size = 1, eccentricity = .9)+
scale_size_area(max_size = 14)+
# scale_color_brewer(palette = "Paired", direction = -1)+
theme_void()
ggsave(filename = "LTNWordCloud.png", path = "C:/Users/Rees Morrison/Documents/R/Projects/LAWYER Hornbooks/5Surveys/", dpi = 300)
postCloud %>%
filter(n > 15) %>%
ggplot(aes(label = word, size = n, col = as.character(n))) +
geom_text_wordcloud(rm_outside = TRUE, max_steps = 1,
grid_size = 1, eccentricity = .9)+
scale_size_area(max_size = 14)+
# scale_color_brewer(palette = "Paired", direction = -1)+
theme_void() +
theme(plot.margin = grid::unit(c(0,0,0,0),"cm"))
ggsave(filename = "LTNWordCloud.png", path = "C:/Users/Rees Morrison/Documents/R/Projects/LAWYER Hornbooks/5Surveys/", dpi = 300)
kable(sentDFExtremes[  , -c(2, 3)])
library(sentimentr)
# https://arxiv.org/pdf/1901.08319.pdf  excellent summary of lexicons and four packages
# It adopts a dictionary lookup approach that tries to incorporate weighting for valence shifters (negators and amplifiers/deamplifiers, which respectively reverse, increase, and decrease the impact of a polarized word).
postSentiment <- posts %>%
unnest_tokens(word, Words) %>%
anti_join(stop_words)
sentDF <- posts %>%
get_sentences() %>%
sentiment()
sentDFHi <- sentDF %>% arrange(desc(sentiment)) %>% top_n(5)
sentDFLow <- sentDF %>% arrange(sentiment) %>% top_n(-5)
sentDFExtremes <- bind_rows(sentDFHi[  , -c(2, 3)], sentDFLow[  , -c(2, 3)])
colnames(sentDFExtremes) <- c("Sentence", "Num. Words", "Polarity Sentiment")
kable(sentDFExtremes[  , -c(2, 3)])
# extract_sentiment_terms(text.var = postSentiment$word)
kable(sentDFExtremes[  , -c(2, 3)]) %>%
row_spec(7, background = "#90ee90")
kable(sentDFExtremes[  , -c(2, 3)]) %>%
row_spec(6:10, background = "#90ee90")
LTNSentKable <- kable(sentDFExtremes[  , -c(2, 3)]) %>%
row_spec(1:5, background = "#90ee90")
kable(sentDFExtremes[  , -c(2, 3)]) %>%
row_spec(1:5, background = "#90ee90") %>%
kable_styling() %>%
save_kable(file = "C:/Users/Rees Morrison/Documents/R/Projects/LAWYER Hornbooks/5Surveys/LTNtable.png")
kable(sentDFExtremes[  , -c(2, 3)]) %>%
row_spec(1:5, background = "#90ee90") %>%
# kable_styling() %>%
save_kable(file = "C:/Users/Rees Morrison/Documents/R/Projects/LAWYER Hornbooks/5Surveys/LTNtable.png")
kable(sentDFExtremes[  , -c(2, 3)]) %>%
row_spec(1:5, background = "#90ee90")
kable(sentDFExtremes[  , -c(2, 3)]) %>%
row_spec(1:5, background = "#90ee90") %>%
# kable_styling() %>%
save_kable(file = "LTNtable.png")
library(knitr)
library(kableExtra)
library(magrittr)
kable(sentDFExtremes[  , -c(2, 3)]) %>%
row_spec(1:5, background = "#90ee90") %>%
# kable_styling() %>%
save_kable(file = "LTNtable.png")
kable(sentDFExtremes[  , -c(2, 3)]) %>%
# row_spec(1:5, background = "#90ee90") %>%
kable_styling() %>%
save_kable(file = "LTNtable.png")
help("save_kable")
kable(sentDFExtremes[  , -c(2, 3)]) %>%
row_spec(1:5, background = "#90ee90") %>%
kable_styling()
LTNKable <- kable(sentDFExtremes[  , -c(2, 3)], caption = "Five Most Positive (Green) and Negative") %>%
row_spec(1:5, background = "#90ee90") %>%
kable_styling()
save_kable(x = LTNKable, file = "LTNtable.png")
install.packages("webshot")
install.packages("webshot")
library(webshot)
save_kable(x = LTNKable, file = "LTNtable.png")
knitr::opts_chunk$set(echo = FALSE, warning=FALSE, message = FALSE)
library(tidyverse)
library(knitr)
library(bookdown)
LTNKable <- kable(sentDFExtremes[  , -c(2, 3)], caption = "Five Most Positive (Green) and Negative") %>%
row_spec(1:5, background = "#90ee90") %>%
kable_styling()
LTNKable <- kable(sentDFExtremes[  , -c(2, 3)], caption = "Five Most Positive (Green) and Negative") %>%
row_spec(1:5, background = "#90ee90") %>%
kableExtra::kable_styling()
library(kableExtra)
LTNKable <- kable(sentDFExtremes[  , -c(2, 3)], caption = "Five Most Positive (Green) and Negative") %>%
row_spec(1:5, background = "#90ee90") %>%
kableExtra::kable_styling()
save_kable(x = LTNKable, file = "LTNtable.png")
kableExtra::save_kable(x = LTNKable, file = "LTNtable.png")
traceback()
kable(sentDFExtremes[  , -c(2, 3)], caption = "Five Most Positive (Green) and Negative Text Comments") %>%
row_spec(1:5, background = "#90ee90") %>%
kableExtra::kable_styling()
library(tm)
library(tidytext)
library(tidytext)
postAssoc <- posts %>%            # at 7:02
unnest_tokens(output = word, input = Words) %>%
anti_join(stop_words, by = "word") %>%
filter(str_detect(word, "[:alpha:]")) %>%
distinct()
View(postAssoc)
View(posts)
postAssoc <- posts[-c(1:11), ] %>%            # at 7:02; 2,840 obs
unnest_tokens(output = word, input = Words) %>%
anti_join(stop_words, by = "word") %>%
filter(str_detect(word, "[:alpha:]")) %>%
distinct()
postAssoc <- posts %>%            # at 7:02; 2,840 obs
unnest_tokens(output = word, input = Words) %>%
anti_join(stop_words, by = "word") %>%
filter(str_detect(word, "[:alpha:]")) %>%
distinct()
mentions <- postAssoc %>%
count(word, name = "users_n") %>%
filter(users_n <= 10)
mentions <- postAssoc %>%
count(word, name = "users_n") %>%
filter(users_n >= 10)
mentions <- postAssoc %>%
count(word, name = "users_n") %>%
filter(users_n >= 5)
mentions <- postAssoc %>%
count(word, name = "users_n") %>%
filter(users_n >= 2)
mentions <- postAssoc %>%
count(word, name = "users_n")
View(mentions)
library(textstem)
postAssoc$lemma <- lemmatize_words(postAssoc$word)
mentions <- postAssoc %>%
count(lemma, name = "users_n") %>%
filter(users_n >= 2)
mentions <- postAssoc %>%
count(lemma, name = "users_n") %>%
filter(users_n >= 3)
View(mentions)
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(readxl)
library(ggthemes)
input <- read_lines("C:/Users/Rees Morrison/Documents/R/Projects/LAWYER Hornbooks/5Surveys/SurveyBook/SavSurveyGuide.Rmd")
one <- str_extract_all(string = input, pattern = "^\\# .+\\#?", simplify = TRUE)
View(one)
rm(one)
blogposts <- str_extract_all(string = input, pattern = "^\\#\\# .+\\#\\#?", simplify = TRUE)
View(blogposts)
head(input)
head(input, 100)
str_extract_all(string = "## Choose a Complementary Team for the Survey Project blah blah ## " , pattern = "^\\#\\# .+\\#\\#?", simplify = TRUE)
str_extract_all(string = "## Choose a Complementary Team for the Survey Project blah blah ## second post ## Choose a Complementary Team for the Survey Project" " , pattern = "^\\#\\# .+\\#\\#?", simplify = TRUE)
str_extract_all(string = "## Choose a Complementary Team for the Survey Project blah blah ## second post ## Choose a Complementary Team for the Survey Project", pattern = "^\\#\\# .+\\#\\#?", simplify = TRUE)
blogposts <- str_extract_all(string = input, pattern = "^\\#\\# .+\\#\\#?", simplify = TRUE)
View(blogposts)
blogposts <- str_extract_all(string = input[1:500], pattern = "^\\#\\# .+\\#\\#?", simplify = TRUE)
input[1:500]
blogposts <- str_extract_all(string = text, pattern = "^\\#{2} .+\\#{2}?", simplify = TRUE)
input <- read_lines("C:/Users/Rees Morrison/Documents/R/Projects/LAWYER Hornbooks/5Surveys/SurveyBook/SavSurveyGuide.Rmd")
# combine all the text, with a space between each term
text <- paste0(input, collapse = " ")
blogposts <- str_extract_all(string = text, pattern = "^\\#{2} .+\\#{2}?", simplify = TRUE)
blogposts <- str_extract(string = text, pattern = "^\\#\\# .+\\#{2}?", simplify = TRUE)
blogposts <- str_extract(string = text, pattern = "^\\#\\# .+\\#{2}?")
blogposts <- str_extract(string = text, pattern = "\\#\\# .+\\#{2}?")
blogposts
blogposts <- str_extract(string = text, pattern = "\\#\\# .+\\#{2} ?")
head(blogposts)
blogposts <- str_extract(string = text, pattern = "\\#\\#\\s.*\\#{2}?")
head(blogposts, 1)
head(text, 1)
blogposts <- str_extract(string = text, pattern = "^\\#\\#\\s.*\\#{2}?")
blogposts <- str_extract(string = "## Test blah blah ## second", pattern = "^\\#\\#\\s.*\\#{2}?")
str_extract(string = "garbage ## Test blah blah ## second", pattern = "^\\#\\#\\s.*\\#{2}?")
str_extract(string = "garbage ## Test blah blah ## second", pattern = "\\#\\#\\s.*\\#{2}?")
str_extract(string = "garbage ## Test blah blah ## second garbage ## Test blah blah ## second", pattern = "\\#\\#\\s.*\\#{2}?")
blogposts <- str_extract(string = text, pattern = "\\#\\#\\s.*?\\#{2}")
str_extract(string = "garbage ## Test blah blah ## second garbage ## Test blah blah ## second", pattern = "\\#\\#\\s.*?\\#{2}")
blogposts <- str_extract_all(string = text, pattern = "\\#\\#\\s.*?\\#{2}")
blogposts <- str_extract_all(string = text, pattern = "\\#\\#\\s.*?\\#{2}", simplify = TRUE)
View(blogposts)
blogposts$V5
blogposts[1, 5]
text <- str_replace_all(text, "\\#\\#\\#", replacement = "zzz")
blogposts <- str_extract_all(string = text, pattern = "\\#\\#\\s.*?\\#{2}", simplify = TRUE)
View(blogposts)
blogposts <- str_extract_all(string = text, pattern = "\\#\\#\\s.*?<!-- End of post -->", simplify = TRUE)
blogposts <- str_extract_all(string = text, pattern = "\\#\\#\\s.*?End of post", simplify = TRUE)
input <- read_lines("C:/Users/Rees Morrison/Documents/R/Projects/LAWYER Hornbooks/5Surveys/SurveyBook/SavSurveyGuide.Rmd")
# combine all the text, with a space between each term
text <- paste0(input, collapse = " ")
text <- str_replace_all(text, "\\#\\#\\#", replacement = "zzz")
blogposts <- str_extract_all(string = text, pattern = "\\#\\#\\s.*?End of post", simplify = TRUE)
View(blogposts)
blogposts <- t(str_extract_all(string = text, pattern = "\\#\\#\\s.*?End of post", simplify = TRUE))
View(blogposts)
blogposts[1, 1]
blogposts <- t(str_extract_all(string = text, pattern = "\\#\\#\\s.*?<!-- End of post", simplify = TRUE))
blogposts[1, 1]
blogposts <- t(str_extract_all(string = text, pattern = "\\#\\#\\s.*?\\<\\!-- End of post", simplify = TRUE))
blogposts[1, 1]
blogposts[ , 1] <- str_remove_all(blogposts[ , 1], "<!-- End of post")
blogposts <- t(str_extract_all(string = text, pattern = "\\#\\#\\s.*?End of post", simplify = TRUE))
blogposts[ , 1] <- str_remove_all(blogposts[ , 1], "<!-- End of post")
blogposts[1, 1]
blogposts$terms <- blogposts[  , 1]
text <- str_replace_all(text, "\\#\\#\\#", replacement = "zzz")
blogposts <- t(str_extract_all(string = text, pattern = "\\#\\#\\s.*?End of post", simplify = TRUE))
blogposts[ , 1] <- str_remove_all(blogposts[ , 1], "<!-- End of post")
colnames(blogposts) <- "PostName"
blogposts$PostName <- str_remove_all(blogposts$PostName, "<!-- End of post")
View(blogposts)
blogposts <- data.frame(t(str_extract_all(string = text, pattern = "\\#\\#\\s.*?End of post", simplify = TRUE)))
colnames(blogposts) <- "PostName"
blogposts$PostName <- str_remove_all(string = blogposts$PostName, pattern = "<!-- End of post")
blogposts$Terms <- blogposts$PostName
blogposts <- blogposts %>%
unnest_tokens(word, Terms) %>%
anti_join(stop_words)
View(blogposts)
input <- read_lines("C:/Users/Rees Morrison/Documents/R/Projects/LAWYER Hornbooks/5Surveys/SurveyBook/SavSurveyGuide.Rmd")
# combine all the text, with a space between each term
text <- paste0(input, collapse = " ")
text <- str_replace_all(text, "\\#\\#\\#", replacement = "zzz")
blogposts <- data.frame(t(str_extract_all(string = text, pattern = "\\#\\#\\s.*?End of post", simplify = TRUE)))
colnames(blogposts) <- "PostName"
blogposts$PostName <- str_remove_all(string = blogposts$PostName, pattern = "<!-- End of post")
blogposts$Terms <- blogposts$PostName  # duplicate the orginal column, so I can remove all but the name of the post in the first column
blogposts$PostName <- str_remove_all(string = blogposts$PostName, pattern = "\\:.*$")
View(blogposts)
blogposts$PostName <- str_remove_all(string = blogposts$PostName, pattern = "\\#\\#")
View(blogposts)
# tokenize each post
blogposts <- blogposts %>%
unnest_tokens(word, Terms) %>%
anti_join(stop_words)
blogposts$Lemmas <- lemmatize_words(blogposts$word)
blogposts <- blogposts %>%
unnest_tokens(word, Terms) %>%
anti_join(stop_words) %>%
filter(str_detect(word, "[:alpha:]")) %>%
distinct()
# tokenize each post  with 26 posts, have 8660
blogposts <- blogposts %>%
unnest_tokens(word, Terms) %>%
anti_join(stop_words) %>%
filter(str_detect(word, "[:alpha:]"))
input <- read_lines("C:/Users/Rees Morrison/Documents/R/Projects/LAWYER Hornbooks/5Surveys/SurveyBook/SavSurveyGuide.Rmd")
# combine all the text, with a space between each term
text <- paste0(input, collapse = " ")
#Replace ### with ZZ so that I don't extract them.
text <- str_replace_all(text, "\\#\\#\\#", replacement = "zzz")
blogposts <- data.frame(t(str_extract_all(string = text, pattern = "\\#\\#\\s.*?End of post", simplify = TRUE)))
colnames(blogposts) <- "PostName"
blogposts$Terms <- blogposts$PostName  # dupe column to remove all but name of post in it
blogposts$PostName <- str_remove_all(string = blogposts$PostName, pattern = "\\:.*$")
blogposts$PostName <- str_remove_all(string = blogposts$PostName, pattern = "\\#\\#")
View(blogposts)
# tokenize each post  with 26 posts, have 8660
blogposts <- blogposts %>%
unnest_tokens(word, Terms) %>%
anti_join(stop_words, by = "word") %>%
filter(str_detect(word, "[:alpha:]"))
blogposts$Lemmas <- lemmatize_words(blogposts$word)
mentions <- blogposts %>%
count(Lemmas, name = "users_n") %>%
filter(users_n >= 3)  # at >=2 = 503; this is 116
View(mentions)
library(widyr)
install.packages("widyr")
posts_who_mention_word <- blogposts %>%
count(Lemmas, name = "users_n") %>%
filter(users_n >= 3)  # at >=2 = 503; this is 116
library(widyr)
word_correlations <- blogposts %>%     # at 8:13 of YouTube
semi_join(mentions, by = "word") %>%
pairwise_cor(item = word, feature = user_name) %>%
filter(correlation >= 0.2)
word_correlations <- blogposts %>%     # at 8:13 of YouTube
semi_join(posts_who_mention_word, by = "word") %>%
pairwise_cor(item = word, feature = user_name) %>%
filter(correlation >= 0.2)
word_correlations <- blogposts %>%     # at 8:13 of YouTube
semi_join(posts_who_mention_word, by = c("word" = "Lemmas"))
word_correlations <- blogposts %>%     # at 8:13 of YouTube
semi_join(posts_who_mention_word, by = c("word" = "Lemmas")) %>%
pairwise_cor(item = word, feature = user_name) %>%
filter(correlation >= 0.2)
word_correlations <- blogposts %>%     # at 8:13 of YouTube
semi_join(posts_who_mention_word, by = c("word" = "Lemmas")) %>%
pairwise_cor(item = word, feature = PostName) %>%
filter(correlation >= 0.2)
posts_who_mention_word <- blogposts %>%
count(Lemmas, name = "users_n") %>%
filter(users_n >= 4)  # at >=3 = 705 obs
word_correlations <- blogposts %>%     # at 8:13 of YouTube
semi_join(posts_who_mention_word, by = c("word" = "Lemmas")) %>%
pairwise_cor(item = word, feature = PostName) %>%
filter(correlation >= 0.3)
library(igraph)
library(ggraph)
graph_from_data_frame(d = word_correlations,   # the edge list, at 9:54 of Youtube
vertices = posts_who_mention_word) %>%
ggraph(layout = "fr") +
geom_edge_link() +
geom_node_point() +
geom_node_text(aes(label = name))
posts_who_mention_word <- blogposts %>%
count(Lemmas, name = "users_n") %>%
filter(users_n >= 5)
word_correlations <- blogposts %>%     # at 8:13 of YouTube
semi_join(posts_who_mention_word, by = c("word" = "Lemmas")) %>%
pairwise_cor(item = word, feature = PostName) %>%
filter(correlation >= 0.4)
graph_from_data_frame(d = word_correlations,   # the edge list, at 9:54 of Youtube
vertices = posts_who_mention_word) %>%
ggraph(layout = "fr") +
geom_edge_link() +
geom_node_point() +
geom_node_text(aes(label = name))  # at 11:32 on YouTube
posts_who_mention_word <- blogposts %>%
count(Lemmas, name = "users_n") %>%
filter(users_n >= 6)
word_correlations <- blogposts %>%     # at 8:13 of YouTube
semi_join(posts_who_mention_word, by = c("word" = "Lemmas")) %>%
pairwise_cor(item = word, feature = PostName) %>%
filter(correlation >= 0.5)
graph_from_data_frame(d = word_correlations,   # the edge list, at 9:54 of Youtube
vertices = posts_who_mention_word) %>%
ggraph(layout = "fr") +
geom_edge_link() +
geom_node_point() +
geom_node_text(aes(label = name))  # at 11:32 on YouTube
graph_from_data_frame(d = word_correlations,   # the edge list, at 9:54 of Youtube
vertices = posts_who_mention_word %>%
#remove words that appear only once at 13:02
semi_join(word_correlations, by = c("word" = "item1"))) %>%
ggraph(layout = "fr") +
geom_edge_link() +
geom_node_point() +
geom_node_text(aes(label = name))  # at 11:32 on YouTube
graph_from_data_frame(d = word_correlations,   # the edge list, at 9:54 of Youtube
vertices = posts_who_mention_word %>%
#remove words that appear only once at 13:02
semi_join(word_correlations, by = c("Lemmas" = "item1"))) %>%
ggraph(layout = "fr") +
geom_edge_link() +
geom_node_point() +
geom_node_text(aes(label = name))  # at 11:32 on YouTube
posts_who_mention_word <- blogposts %>%
count(Lemmas, name = "users_n") %>%
filter(users_n >= 8)
word_correlations <- blogposts %>%     # at 8:13 of YouTube
semi_join(posts_who_mention_word, by = c("word" = "Lemmas")) %>%
pairwise_cor(item = word, feature = PostName) %>%
filter(correlation >= 0.5)
graph_from_data_frame(d = word_correlations,   # the edge list, at 9:54 of Youtube
vertices = posts_who_mention_word %>%
#remove words that appear only once at 13:02
semi_join(word_correlations, by = c("Lemmas" = "item1"))) %>%
ggraph(layout = "fr") +
geom_edge_link() +
geom_node_point() +
geom_node_text(aes(label = name))  # at 11:32 on YouTube
# from YouTube https://www.youtube.com/watch?v=ae_XVhjHd_o  code below at 7:24
posts_who_mention_word <- blogposts %>%
count(Lemmas, name = "users_n") %>%
filter(users_n >= 9)  # at 3 = 705; at 4 = 519; at 5 = 391; at 6 310; 8 = 213
word_correlations <- blogposts %>%     # at 8:13 of YouTube
semi_join(posts_who_mention_word, by = c("word" = "Lemmas")) %>%
pairwise_cor(item = word, feature = PostName) %>%
filter(correlation >= 0.6)  #27,652 at 0.3; at 0.4 8764 obs; at 0.5 and 8 post = 2214
graph_from_data_frame(d = word_correlations,   # the edge list, at 9:54 of Youtube
vertices = posts_who_mention_word %>%
#remove words that appear only once at 13:02
semi_join(word_correlations, by = c("Lemmas" = "item1"))) %>%
ggraph(layout = "fr") +
geom_edge_link() +
geom_node_point() +
geom_node_text(aes(label = name), repel = TRUE)  # at 11:32 on YouTube; up to 13:37 repel
ggsave(filename = "LTNWordAssoc.png", path = "C:/Users/Rees Morrison/Documents/R/Projects/LAWYER Hornbooks/5Surveys/", dpi = 300)
graph_from_data_frame(d = word_correlations,   # the edge list, at 9:54 of Youtube
vertices = posts_who_mention_word %>%
#remove words that appear only once at 13:02
semi_join(word_correlations, by = c("Lemmas" = "item1"))) %>%
ggraph(layout = "fr") +
geom_edge_link(aes(alpha = correlation)) +  # darker the edge, higher the corr
geom_node_point() +
geom_node_text(aes(color = users_n, label = name), repel = TRUE)  # at 11:32 on YouTube; up to 13:37 repel; lighter blue the text, the more posts contain the word
ggsave(filename = "LTNWordAssoc.png", path = "C:/Users/Rees Morrison/Documents/R/Projects/LAWYER Hornbooks/5Surveys/", dpi = 300)
