---
site: blogdown:::blogdown_site
---

<!-- This file is for blogdown only. Please do not edit it. -->

<!--chapter:end:index.Rmd-->

---
title: "Workflow for Blog Savvy Surveys for Lawyers"
subtitle: "in BlogSavSurv.Rmd Hornbooks/5Surveys/LFSurveys/Survey Projects Marketing/SurveyBlog/"
author: "Rees Morrison"
date: "`r Sys.Date()`"
output:
  pdf_document: default
  html_document: default
fig_caption: yes
urlcolor: blue   
---

```{r setup, include=FALSE, echo=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning=FALSE, message = FALSE)
```

```{r packages, echo=F, message=F}
library(tidyverse)  # frequently used packages that follow a similar coding approach
library(blogdown)
library(knitr)
library(funModeling) # to compare vectors against each other, such as URLs of posts
library(ggthemes) # to include the theme_tufte them in plots
library(lubridate) # to deal with date data, such as the date a post is published
library(ggrepel) # to spread points or text so that they do not overlap on a plot
library(readxl) # to read in the underlying Excel data
library(tidytext) # text-mining functions that help analyse the content of posts
library(writexl) 
library(kableExtra)

```


<!-- CHEAT SHEET https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet -->

# How to create individual posts

Start RStudio; Open the SurveyBlog project and load packages; call serve_site() [Need to be in the right directory, Hornbooks/5Survey etc.  I shut down R and started afresh and the right directory followed the project]

Click on New Post dropdown from Addins, which runs this: blogdown:::new_post_addin().

<!-- https://alison.rbind.io/post/2017-06-12-up-and-running-with-blogdown/ -->

  Name the post: 
  Category  Surveys (or maybe new line indented - Surveys)
  Tags delete 
  Ignore Slug 
  output: pdf_document   should I add this?
  With the radio button at the bottom keep Format: Markdown
  
  File/Rename [core idea, no spaces] and save the new .md.
  draft: replace "yes" with "no" -- no quote marks

KEY delete the index file from SurveyBlog/content/post for the post to publish (use .Rmd with code)

  Copy and paste in the text 
  
  Add packages chunk at the top if there is R code
  
  Add data chunk to read in, for example, ukrAll
  
  Remove \newpage
  
  Make changes on my laptop in the .md file  Useful site https://www.spanishdict.com/answers/152876/line-breaks-pictures-links-and-other-formatting-tricks-#Single
  
insert at end of post <!-- End of post -->

## Pushing posts to GitHub, which then sends them to Netlify and the blog 

Connect to GH: click "Git" above Pane 3, and "Commit" button (check mark, CTRL-ALT-M)

Stage files listed on the left pane by checking, which is usually all of them to either add or delete [D].  Ctrl-A picks all and then click one

Fill in Comment of what has been done, or any random typing

[click on box under "Staged" in Git at top right], Commit to GH

Push to GH [still "branch is one commit ahead of main"].  "Stop"

Then click "Close"

Render is the hosting software.  To make sure Render is working: Go to render.com and login.  Manual Deploy in purple box to upper right -- takes 15 seconds and should end with "==> Your site is live!"

Tracking hits is https://savvysurveys.goatcounter.com/


# How to put blog posts into the Guide

Open SavSurvGuideLean.Rmd file in 5Surveys/surveyBook when in project what?

insert colon at end of title

Put in R chunk (but it must be an RMD:

knitr::include_graphics("C:/Users/rees/Documents/R/Projects/LAWYER Hornbooks/5Surveys/SavSurvBlog/SurveyBlog/static/media/RStudio Screenshot Scripts Post.png")

If the image isn’t generated by R, fig.width and fig.height are redundant
Use PNG for R graphics output, and high-resolution if you plan to print.

&nbsp;&nbsp;&nbsp;  [three &nbsp] indents a paragraph

turn hyperlinks in text into footnotes for PDF output, because readers will not be able to click the links (generated from \href{URL}{text}).  See https://bookdown.org/yihui/rmarkdown-cookbook/latex-preamble.html

blogdown creates a file structure in XX/ that includes these folders:

  content (where blog is)
  layouts (css and js) # https://makewithhugo.com/minify-and-load-css-through-hugo/
  R [haven't used]
  resources [haven't used]
  static (media) .pngs and other images

  Check for bold of key terms, using **...**
  Insert images with the Addins drop down
  Insert URLs as [underlined text](URL)   NB no space
<!-- Insert images from the static/media subfolder using ![](/media/picture_vacations.png)  or .jpg -->
  {width=50%, height=20%} didn't work in .md files but does in .Rmd.  Note that there is no space.  Opened image in Paint and resized to 70.  So I right clicked on the image and chose "Edit" and resized.  With the Absinthe painting the original was about 650px by 850.  I reduced it to 60% and saved, but no difference when I ran the post. 
  No quotes on file path, no need for directories above BlogThemes1.  Underline turns blue when I do correct syntax.  
  
    Run Check Spelling… in RStudio (under Edit menu)
    Run xfun::optipng() on the directory where the post’s images are stored (including plots output by R) for lossless reduction of the size of .png images  [RWM for jpg?]
    Run W3C Link Checker on the post preview to make sure all links are valid.
    Check images have alt text. A web accessibility evaluation tool can help.  [https://www.w3.org/WAI/ER/tools/]

  Save the .md file and check that the changes took (then commit to GitHub).  You can immediately view changes with every save using LiveReload and your .html file is properly output. Changes are local.  Viewer shows what a user sees on the site. 

  Save the URL of post on blog by right clicking title and "Copy link location"; save the long URL in this file
  Get the Bitly short form and add it after the full URL (on Bitly, click "Create" orange tab, then "Link".  Paste in long form and click "copy" to save short form; save below long form)
  
blogdown::check_site() will run all check_*() functions at once. 
    
config.toml has many options, such as changing the name of the site!

\newpage

# Creating a blog with Hugo, blogdown, Netlify

## changes to the blog site

Hugo builds the website pages out of blocks of code. If you are using a straightforward theme like I am then the file at: /hugo_root/themes/theme/layouts/_default/single.html

is the file that builds the pages. This does not contain most of the HTML that will end up in the final page, rather, it contains calls to other files that contain the needed HTML. Notably, the header.html and footer.html files.

If we want to add some custom HTML into the page header then we will need to add it to the header.html or include a call to another file that does contain the HTML. We will do that latter as it will be much easier to keep everything neat if we put the Google Analytics code in its own file.

useful:  rstudioapi::navigateToFile("config.yaml") or pick any file

Under /Themes/BlogThemes found config.yaml and changed a number of features, including title, author, some page names

/BlogThemes/themes/hugo-tanka (changed in early April 2021 to hugo-texify)

much useful: https://alison.rbind.io/post/2019-02-19-hugo-archetypes/
also http://bioinfohippo.rbind.io/tutorials/r_tutorials/2020_05_20_0847-r_blogdown_usefulstuff/
http://bioinfohippo.rbind.io/tutorials/r_tutorials/2020_05_20_0847-r_blogdown_tutorial/
http://estebanmoro.org/post/2019-02-02-setting-up-your-blog-with-rstudio-and-blogdown-i-creating-the-blog/

https://www.r-bloggers.com/2019/01/how-i-started-a-blog-based-on-blogdown-a-walkthrough/

look in Themes by using Files in Pane 4, and find config.  One is params.toml

When Hugo builds the blog, the final html files and structure go into the public folder. This is the static version of the blog and the one we will deploy to our domain.

```{r blogSetup}
# new_site(theme = "nanxstats/hugo-tanka", force = TRUE)  
# FEATUREs of Hugo Tanka at https://awesomeopensource.com/project/nanxstats/hugo-tanka

# try URL 'https://github.com/wowchemy/wowchemy-hugo-modules/archive/25e0b0627f09.tar.gz'
# Content type 'application/x-gzip' length unknown
# downloaded 491 KB

# blogdown::stop_server() or restart the R session.  To stop it, call 

```

    
### Learn about HUGO
https://ropensci.org/blog/2019/01/09/hugo/

### Blog color scheme colorBrewer palette Dark2https://www.w3schools.com/css/tryit.asp?filename=trycss3_flexbox_justify-content_space-around   Try out your code

.customBlogTitle in index.css  h1   and header.html
navbarLink in common.css       h2   and header.html and header.html, I changed to "center"
.customPostTitle in index.css  h3
subthemes in common.css        h4

add extra line
To add an extra line of space between paragraphs, add the HTML &nbsp; code, followed by two extra spaces (e.g. &nbsp.., replacing the periods with spaces).

I used this: https://colorbrewer2.org/#type=qualitative&scheme=Dark2&n=3

27,158,119  dark green for Blog Title  
217,95,2    orangy-red for tabs ()
117,112,179 purplish for blog titles   
            subthemes link
            
### Blog text font size, color and css cascading style sheets
Note: If you do not specify a font size, the default size for normal text, like paragraphs, is 16px (16px=1em).

BlogTitle
Navbar
PostTitle

### Tabs at top of blog (Navbar): Services

about.md to modify the About tab. 

services.md for my selling pitch 

# Sidebar to list related blogs
https://discourse.gohugo.io/t/how-would-i-create-a-sidebar-with-page-specific-related-links/9037/4

### create a form
I used Formspree to make a contact form, which is an online service (managed on GitHub) that allows you to add an HTML form to your static site. No registration, just use the form and confirm your email address once. I added the following code into my contact widget: 
https://alison.rbind.io/post/2017-06-12-up-and-running-with-blogdown/

## Create box for icons
config.toml has in it the elements below the blog title, such as About, Art, Themes, and Subthemes.  They have weights that determine their order.

I created an Icon element to practice boxes and made it weight 5

In BlogThemes1/content I created icon.Rmd

# domain names of savvysurveysjd.com is with namecheap; expires May 21, 2024

## One-time steps for GH, NetLify etc.  

<!-- Created GH repository ("BlogThemes"), joined NetLify and linked GH to it, installed blogdown package, picked the HugoTanka theme (modified it, later switched to TeXify) -->

Later, created GH repository SurveyBlog.  Linked it to Render [Your service is always available at https://surveyjds.onrender.com.].  But in GH, /content/post shows posts in reverse chrono order

https://alison.rbind.io/post/new-year-new-blogdown/   [Has three other posts and is excellent!]  She wrote the following before:  https://alison.rbind.io/post/2020-12-27-blogdown-checks/

    I am strongly in favour of never using .Rmd in a blogdown site because its output is an html file, not a .markdown/.md file. from  [RWM but what about inline code?] https://masalmon.eu/2020/02/29/hugo-maintenance/
 
    use .md so Hugo converts to html, but need to edit profile  https://drmowinckels.io/blog/2020-05-25-changing-you-blogdown-workflow/  also need to add archetype to YAML
    Third post Alison cites is too complicated https://clauswilke.com/blog/2020/09/08/a-blogdown-post-for-the-ages/


### Google Analytics

https://statsandr.com/blog/track-blog-performance-in-r/   Antoine!

What R package handles Google analytics data?  This one lists several others!  do a comparison?
https://code.markedmondson.me/googleAnalyticsR/

Account Id
188088269 (which is different from the User Authorization code)

```{r googleAnalytics, eval=FALSE}
library(googleAnalyticsR)

# https://www.rubenvezzoli.online/googleanalyticsr-beginners-guide/

# Connect R and Google Analytics: I chose Allow across all of them
# ga_auth()

# Generate a list of all the Google Analytics accounts you have access to
# Account Id
# 188088269
# Account Name
# BlogThemes

# ga_accounts <- ga_account_list()   This picked up Juris Datoris but not Netlify
# > ga_account_list()
# Auto-refreshing stale OAuth token

# VIEW ID ga_id contains the View ID that you want to query
ga_id <- 188088269


# PROPERTY ID  in Admin/Property Settings, which I searched for after online research
# 268288914
# ga_id <- 268288914

# above didn't work so I search for Profile ID on GA site
# ga_id <- "G-WPRZXLGMHX"
# Error: API returned: Invalid value 'ga:G-WPRZXLGMHX' for viewId parameter.

# fourth effort, based on this SO Question
# https://stackoverflow.com/questions/14525565/not-sufficient-permissions-google-analytics-api-service-account
# Same issue. Solved it by using the VIEW id instead of the account Id (UA-XXXXX-1).
# 
# Analytics Console > Admin -> View (Profile) -> View Settings -> View ID
# 
# Make sure that you add the service account to your list of users in the Google Analytics Console. Simply setting it up in the Credentials, API and Permissions section of the developer console will still not grant it access to your analytics.
# 
# Analytics Console > Admin -> Account -> User Management -> "Add permissions for:"  RWM what is the service account I need to add?

# Download the data and store them in a dataframe
# ga_results1 <-google_analytics(viewId = ga_id,
#                                date_range = c("2020-05-01", "2020-05-12"),
#                                metrics = c("users","sessions"),
#                                dimensions = "date")
# Request Status Code: 403
# Error: API returned: User does not have sufficient permissions for this profile. when I tried YTD, and May

```

Google Analytics Export Data and Analyze

```{r GAReports, echo=FALSE, eval=FALSE}
# Pick the custom date range, starting April 11, 2021.  Upper right, click on Share this report (middle icon) and export CSV, but save it as XLSX in BlogThemes1

GAData <- readxl::read_xlsx(path = "C:/Users/Rees Morrison/Documents/R/Projects/CLIENTS/Themes/BlogThemes1/GoogleAnalyticsExport.xlsx")

# I think all the numbers are New Users
GAData <- GAData[18:107, 1:4]
colnames(GAData) <- c("Day", "none", "referral", "organic")
GAData$none <- as.numeric(GAData$none)
GAData$referral <- as.numeric(GAData$referral)
GAData$organic <- as.numeric(GAData$organic)
GAData$NewUsers <- GAData$none + GAData$referral + GAData$organic


```

## analyze Bitly data with r

```{r bitly, eval=FALSE}
# library(urlshorteneR)  # this shortens URLs, but doesn't help with bitly
# bitly_auth()

# https://stackoverflow.com/questions/11493425/accessing-the-bitly-oauth2-api-from-r   maybe overkill for me

bitlyAll<- read_xlsx("C:/Users/Rees Morrison/Documents/R/Projects/CLIENTS/Themes/Bitly data.xlsx", sheet = "transpose2")

# colnames(bitlyAll) <- rep(c("Date", "Post", "URL", "Clicks"), 124)  # I named in Excel, which autonumbered
# bitlyAll <- bitlyAll %>% filter(seq(1,496,4), seq(3,496,4))

bitlyAll <- bitlyAll[1, -c(seq(1,496,4), seq(3,496,4))]  # remove bitly URL and date

# bitly <- pivot_longer(data = bitlyAll, names_to = "name", values_to = "value",
#                       cols = seq(1,248))

bitlyDF <- data.frame(Posts = t(bitlyAll[1, seq(1, 248, 2)]), Clicks = t(bitlyAll[1, seq(2, 248, 2)]))

```

# EMAIL notices emayili

https://mvaugoyeau.netlify.app/post/presentation-off-women-international-day/#sending-the-support-with-the-package-sendmailr

https://bensstats.wordpress.com/2020/07/23/robservations-2-mail-merging-email-blasting-with-r/ with emayili

A web browser uses HTTP (Hypertext Transfer Protocol) to communicate with a web server. Similarly, an email client (like Outlook or Thunderbird) uses SMTP (Simple Mail Transfer Protocol) as the communication protocol to send emails.

```{r email, eval=FALSE}
# emayili sending email with minimal dependencies. https://datawookie.github.io/emayili/
library(emayili)

# First create a message object.  Start here and delete the existing email, or it will repeat text
email <- envelope()

# Add addresses for the sender and recipient.
email <- email %>%
  from("rees@reesmorrison.com") %>%
  to("rees@reesmorrison.com", "iamwillmorrison@gmail.com")

# There are also bcc() and reply() functions for setting the Bcc and Reply-To fields.

# Add a subject.
email <- email %>% subject("Sending to Two!")

# Add a text body. You can use html() to add an HTML body.
email <- email %>% text("The latest topic covers Time.  It draws on a poem by Andrew Marvell, a movie with Bill Murray, a painting by Degas, and a song by The Byrds.  Please let me know your thoughts and observations.")

# Add an attachment.
# email <- email %>% attachment("image.jpg")  # excel file?

# Create a SMTP (Simple Mail Transfer Protocol) server object and send the message.
# How do I get the password and is my gmail address my user name
# More information about this service can be found at https://www.smtpbucket.com/.
# Google's server listens for secure connections on ports 465 and 587.

# email

smtp <- server(host = "smtp.gmail.com",
               port = 465,  # the SSL port
               username = "rees@reesmorrison.com",
               password = "vlmaxexlnnfpjtkn")   # we set up 2FA (factor authentication with my gmail account and it generated this random password)

# If you are using SSL, the port number is usually 465 and for TLS the port is usually 587. Even so, it's very easy to get hung up on the server details… Especially, when you try both and it doesn't work. This is most common with the major webmail services (e.g, Gmail, Yahoo, etc), because they use a different protocol by default, STARTTLS. 

smtp(msg = email)  # error  Login denied
# 535-5.7.8 Username and Password not accepted. Learn more at
# 535 5.7.8  https://support.google.com/mail/?p=BadCredentials i5sm17373653qkg.32 - gsmtp

# Simply printing a message displays the header information.
email

# qp_encode(c(Rees, Will))

# You can identify emails which have been sent using {emayili} by the presence of an X-Mailer header which includes both the package name and version.
# 
# To see the guts of the message as passed to the SMTP server:
#   print(email, details = TRUE)
# 
# Using STARTTLS
# 
# If you're trying to send email with a host that uses the STARTTLS security protocol (like Google Mail), then it will most probably be blocked due to insufficient security. In order to circumvent this, you can grant access to less secure apps. See the links below for specifics:
#   
#   Google (details)
# 
 
# other R packages which also send emails:
  
# blastula
# blatr (Windows)
# gmailr
# mail
# mailR
# sendmailR
# ponyexpress
```


```{r packagegmailr, eval=FALSE}
# Sending Messages With Gmailr"

library(gmailr)

## Constructing a MIME message
## Text First we will construct a simple text only message

text_msg <- gm_mime() %>%
  gm_to("rees@reesmorrison.com") %>%
  gm_from("rees@reesmorrison.com") %>%
  gm_text_body("Gmailr is a very handy package!")

# You can convert the message to a properly formatted MIME message using `as.character()`.
# Multipurpose Internet Mail Extensions (MIME) is an Internet standard that extends the format of email messages to support text in character sets other than ASCII, as well as attachments of audio, video, images, and application programs.
# 
# ```{r sending_messages_simple_print}
# strwrap(as.character(text_msg))

# You can also construct html messages.  It is customary to provide a text only message along with the html message, but with modern email clients this is not strictly necessary.

# html_msg <- gm_mime() %>%
#   gm_to("james.f.hester@gmail.com") %>%
#   gm_from("me@somewhere.com") %>%
#   gm_html_body("<b>Gmailr</b> is a <i>very</i> handy package!")

### Attachments

# You can add attachments to your message in two ways.
# 
# 1. If the data is in a file, use `gm_attach_file()`.  The mime type is automatically guessed by `mime::guess_type`, or you can specify it yourself with the `type` parameter.

# write.csv(file = "iris.csv", iris)
# 
# msg <- html_msg %>%
#   gm_subject("Here are some flowers") %>%
#   gm_attach_file("iris.csv")


# 2. If the data are already loaded into R, you can use `gm_attach_part()` to attach the binary data to your file.
# sending_messages_attachments_1}
# <!-- msg <- html_msg %>% -->
# <!--   gm_attach_part(part = charToRaw("attach me!"), name = "please") -->


# <!-- ### Including images -->

# <!-- You can also add use attached images in HTML by setting the Content ID feature of mime emails. This can be done by referencing the image via a `<img -->
# <!-- src=cid:xyz>` tag using the `id` argument of `send_file()`. The tag value can by any unique identifier. E.g. here is an example of including a ggplot2 image -->

# First create a plot to send, and save it to mtcars.png
# mtcars$gear <- as.factor(mtcars$gear)
# 
# png("mtcars.png", width = 400, height = 400, pointsize = 12)
# with(mtcars,
#      plot(hp,
#           mpg,
#           col = as.factor(gear),
#           pch = 19,
#           xlab = "Horsepower",
#           ylab = "Miles / gallon"
#      )
# )
# legend("topright", title = "# gears",
#        pch = 19,
#        col = seq_along(levels(mtcars$gear)),
#        legend = levels(mtcars$gear)
# )
# dev.off()

# Next create an HTML email that references the plot as 'foobar'
# email <- gm_mime() %>%
#   gm_to('someaddress@somewhere.com') %>%
#   gm_from("someaddress@somewhere.com") %>%
#   gm_subject("Cars report") %>%
#   gm_html_body(
#     '<h1>A plot of <b>MotorTrend</b> data <i>(1974)</i></h1>
#     <br><img src="cid:foobar">') %>%
#   gm_attach_file("mtcars.png", id = "foobar")

# unlink("mtcars.png")

## Uploading
### Create Draft

# You can upload any mime message into your gmail drafts using `gm_create_draft()`.  Be sure to give yourself at least `compose` permissions first.

#sending_messages_create_draft, eval=FALSE}
# gm_create_draft(file_attachment)

### Insert

# This inserts the message directly into your mailbox, bypassing gmail's default scanning and classification algorithms.

# gm_insert_message(file_attachment)

### Import

# This imports the email as though it was a normal message, with the same scanning and classification as normal email.

```{r sendingfileattachment, eval=FALSE}
# gm_insert_message(file_attachment)
```

```{r sendmailr, eval=FALSE}
library(sendmailR)

from <- sprintf("rees@reesmorrison.com","Rees Morrison") # the sender’s name is an optional value
to <- sprintf("iamwillmorrison@gmail.com")
subject <- "Test email subject"
body <- "Test email body"

sendmail(from,to,subject,body,control=list(smtpServer="smtp.gmail.com"))
# Error in if (code == lcode) { : argument is of length zero

```

```{r mailr}
#library(mailR)  # requires Java
```

## Sending Message

You can also send an email message directly from a `mime` object using `gm_send_message()`.

gm_auth_configure()
gm_send_message(text_msg)


```{r sending_messages_clenup, include=FALSE}
unlink("iris.csv")
```

## Six Ways to plot binary

```{r binary, eval=FALSE}
binary <- data.frame(Answer = c("Yes", "No"),
                     Pct = c(0.60, 0.40))

column <- ggplot(binary, aes(x = Answer, y = Pct)) + 
  geom_col(width = 0.7) +
  labs(x = "Answer", y = "Pct.", title = "Bar Chart") +
  scale_y_continuous(breaks = seq(0, 1, 0.2), labels = scales::percent_format())
column


stacked <- ggplot(binary, aes(x="", y=Pct, fill=Answer)) +
  geom_bar(width = 1, stat = "identity")
stacked 

pie <- ggplot(binary, aes(x = "", y = Pct, fill = Answer)) + 
  geom_bar(width = 1, stat = "identity") +
  coord_polar("y", start = 0) +
  labs(x = "Answer", y = "Pct.", title = "Bar Chart") +
  scale_y_continuous(breaks = seq(0, 1, 0.2), labels = percent_format())
pie

fan.df <- data.frame(Answers = c(rep("Yes", 20), rep("No", 8)))

# library(ggfan)  # doesn't look like a fan plot
# fan <- ggplot(fan.df, aes(x = Answers)) + 
#   geom_fan(intervals = intervals) +
#   labs(x = "Answer", y = "Pct.", title = "Fanplot") +
#   scale_y_continuous(breaks = seq(0, 1, 0.2), labels = percent_format())
# fan

```

Adherents of fanplots maintain that they are a better alternative to pie charts. They represent amounts of values.  The traditional pie plot is difficult for most people to interpret, because humans are not trained well to observe and interpret angles.  This fan plot was generated by the plotrix package and its fan.plot function.  

```{r fanplot, eval=FALSE}
library(plotrix)   # for fanplot of DataVisualizations

fan.plot(x = binary$Pct, max.span = pi, main = "Fanplot", labels = TRUE)

```

<!--chapter:end:BlogSavSurv.Rmd-->

---
title: "ImputationPractice.Rmd in SurvSurvBlog"
author: "Rees Morrison, Esq."
date: "`r Sys.Date()`"
output: pdf_document
---


```{r setup, include=FALSE, echo=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning=FALSE, message = FALSE)
```

```{r packages, echo=F, message=F}
library(tidyverse)  
library(blogdown)
library(knitr)
library(funModeling) # to compare vectors against each other, such as URLs of posts
library(ggthemes) # to include the theme_tufte them in plots
library(lubridate) # to deal with date data, such as the date a post is published
library(ggrepel) # to spread points or text so that they do not overlap on a plot
library(readxl) # to read in the underlying Excel data
library(tidytext) # text-mining functions that help analyse the content of posts
library(writexl) 
library(kableExtra)

library(cowplot)

```


# Correlation part two

The Pearson correlation corresponds to the covariance of the two variables normalized (i.e. divided) by the product of their standard deviations.  

The Spearman correlation between two variables is equal to the Pearson correlation between the rank scores of those two variables; while Pearson’s correlation assesses linear relationships, Spearman’s correlation assesses monotonic relationships (whether linear or not). A monotonic relationship between two variables refers to a scenario where a change in one variable is generally associated with a change in a specific direction in another variable. This may be linear or non-linear.

Kendall’s Rank Method:

In the normal case, the Kendall correlation is preferred to the Spearman correlation because of a smaller gross error sensitivity (GES) and a smaller asymptotic variance (AV), making it more robust and more efficient. Kendall rank correlation is used to test the similarities in the ordering of data when it is ranked by quantities. Other types of correlation coefficients use the observations as the basis of the correlation, Kendall’s correlation coefficient uses pairs of observations and determines the strength of association based on the patter on concordance and discordance between the pairs.

Biweight midcorrelation:

A measure of similarity that is median-based, instead of the traditional mean-based, thus being less sensitive to outliers. Therefore, if dataset and features have much outliers, this is a better correlation to use instead of other correlations.

Distance correlation is a measure of association strength between non-linear random variables. It goes beyond Pearson’s correlation because it can spot more than linear associations and it can work multi-dimensionally.

```{r correlation}
# https://medium.com/@danishaman202/correlation-methods-c79b48286908  python
# https://medium.com/data-science-in-your-pocket/calculating-non-linear-correlation-using-distance-correlation-with-examples-83f6a8cf2473

# https://universeofdatascience.com/16-different-methods-for-correlation-analysis-in-r/

d2c <- readxl::read_xlsx("C:/Users/rees/Documents/R/Projects/CLIENTS/LFExecDirs/US Law Firms/d2cEleven.xlsx")

d2c <- d2c %>% select("Lawyers", "Offices") %>% filter(Lawyers > 5 & Offices > 0)  # 1322

# covariance
covLawOffice <- cov(d2c$Lawyers, d2c$Offices) # 3,544
sdLawyers <- sd(d2c$Lawyers)
sdOffices <- sd(d2c$Offices)

# Pearson correlation is covariance divided by product of SD
# covLawOffice/(sdLawyers * sdOffices)  0.79489
cor(d2c$Lawyers, d2c$Offices, method = "pearson")   # 0.79489

# spearman
cor(d2c$Lawyers, d2c$Offices, method = "spearman") # 0.628

# Kendal
cor(d2c$Lawyers, d2c$Offices, method = "kendal")  #0.4759

# see post above with 16, and correlation package
library(correlation) 

#biweight correlation, a robust alternative to Pearson correlation coefficient
correlation(d2c, method = "biweight", p_adjust = "none")

# can "plot" each one
plot(correlation(d2c, method = "biweight", p_adjust = "none"))

# Distance correlation
correlation(d2c, method = "distance", p_adjust = "none")

```

```{r missingValues, eval=FALSE}
library(finalfit)

missingPlot <- dfReports %>% finalfit::missing_plot(plot_opts = theme(panel.background = element_rect(fill = "grey30")))
missingPlot

```


# Imputation

relaimpo package, it seems it can handle multiply imputed datasets using the function mianalyze.relimp()

Dealing With NAs in R: From Deletion to Replacing to Machine Learning by Martinqiu
New idea for imputation. Create my own regression model, and use that to predict missing values.

Imputation in R: Top 3 Ways for Imputing Missing Data by Dario Radečić

ISRS is imputation package and I have a description of it

If you do not choose to delete observations that have missing data, you can turn to imputation techniques.  If you impute a single variable (univariate) that is **numeric**, you can use various imputations:

* Mean/median 
* Arbitrary value
* End of distribution
* Random value
* Replace with previous/next value

For replacement of missing values based on more than one variable,

* KNN 
* MICE (Multivariate Imputation by Chained Equations)

with KNN imputation, the basic idea is that any data point in a dataset is similar to its neighbors. The similarity is calculated by the euclidean distance between data points in a n-dimensional space, where n is the number of independent features. But in KNN Imputer class the distance is calculated using nan euclidean distance. This distance takes care of any missing values in the K neighbors while calculating distance.

With MICE, The following steps are involved in MICE:

Step 1: Fill the missing values either with a random value from the column or with the mean of the values of that column. Let’s name this Table as Base Table.

Step 2: Going from left to right, remove missing values from one column and take that column (with missing values) as dependent variable and others as independent variable and use a machine learning model to predict the missing values.

Step 3: Repeat the process for other columns having missing values. We will obtain a table after predicting all the missing values. Let’s name this table as Table 1.

Step 4: Once the missing values are all imputed, take the base table (the one obtained by imputing missing values with mean) and the table obtained after predicting the values through a ML model (Table 1) and calculate the difference between the two. Let’s call it the difference table.

Step 5: Now, take Table 1 as the base table and repeat steps 2,3 and 4. Here, the difference would be between Table 1 and the new table obtained after iteration (say Table 2).

Step 6: Repeat the iterations until all the values in the difference table nearly reach 0. The table obtained in the last iteration would have the most accurate values for the missing data.
<!-- imputation by linear regression with mice package   https://medium.com/nerd-for-tech/imputing-missing-data-with-r-a7f04692c895 -->
<!-- Should I even try this? It feels like what regression does to predict numbers. -->
<!-- https://towardsdatascience.com/imputation-of-missing-data-in-tables-with-datawig-2d7ab327ece2 -->

https://medium.com/@ayushmandurgapal/data-preprocessing-handling-missing-values-in-a-dataset-5140f77d2a47


```{r dataMissingSkimr, skimr_digits = 2, eval=FALSE}
# identify where missingness is
library(naniar)

ggplot(data = dfReports, aes(x = Total, y = Reports)) +
  geom_miss_point(colour = c("black", "red")) +
  theme_tufte()
```



```{r data}
# from https://appsilon.com/imputation-in-r/
imp <- readxl::read_xlsx("C:/Users/rees/Documents/R/Projects/CLIENTS/LFExecDirs/combo58.xlsx")

#keep only numerics and a few of them
imp <- imp[    , c("YearsRole", "Reports", "Lawyers", "Base", "Bonus", "Total")]  # 225 obs

imp$Base[61] <- NA
imp$Base[124] <- 114000
imp$Bonus[124] <- 12000
imp$Total[124] <- 126000
imp$Bonus[142] <- 10000

```

Imputation is such a powerful tool for analyses of survey data, because it allows **machine learning** algorithms to chew through larger databases, analysts have at their disposal several imputation methods.

We can explore those methods and how their results differ, by drawing on the survey of Chief Operating Officers in U.S. law firms.  One question asked them to state their base salary and another asked for their bonus.  Of 225 responses, 20 did not comply.  However, we can impute the total compensation (base plus bonus) and test the differences in the imputed distribution and the original distribution.

Before plunging in, we need to wave a red flag.  Reliable imputation is almost always tied to domain knowledge of the problem you’re trying to solve, so make sure to ask the right business questions.  If, for example, the three missing values all came from respondents with the title "Chief *Executive* Officer," you might look deeper to see whether that title has an odd connection with numbers of direct reports.  More generally, take pains to make sure that no pattern explains the missing values.

First, let's peek at the number and pattern of missing values (NA in R terminology).

```{r, mice}
library(mice)
md.pattern(imp, rotate.names = TRUE)

```

Inspect the histogram of your original data that you would like to fill in by imputation, before you carry out any imputation.  You will want to compare its general shape with the shapes created after the various methods of imputation.  In each of the imputation models shown below, the original distribution of the Reports variable sits in the upper left pane.

```{r histoYears}
# summary(imp$YearsRole)  # no nas
# summary(imp$Base)  # 3 NAs
# summary(imp$Lawyers) # no NAs

# ggplot(imp, aes(Reports)) +
#   geom_histogram(color = "#000000", fill = "#0099F8") +
#   ggtitle("Variable distribution") +
#   theme_classic() +
#   theme(plot.title = element_text(size = 18)) +
#   scale_y_continuous(breaks = seq(1, 60, 4)) +
#   scale_x_continuous(breaks = seq(1, 50, 5))

```

\newpage

Now, compare the imputed-value distributions to the original distribution (top left).  They look identical.  I used a constant of 8 because the original histogram suggests that 8 represents more of the answers than any other value.

```{r imputedMeanetc, message=FALSE}
library(mice)

# constant of 8 was from eyeballing histogram
value_imputed <- data.frame(
  original = imp$Total,
  imputed_zero = replace(imp$Total, is.na(imp$Total), 200000),  # plausible value
  imputed_mean = replace(imp$Total, is.na(imp$Total), mean(imp$Total, na.rm = TRUE)),
  imputed_median = replace(imp$Total, is.na(imp$Total), median(imp$Total, na.rm = TRUE))
)

h1 <- ggplot(value_imputed, aes(x = original)) +
  geom_histogram(fill = "#ad1538", color = "#000000", position = "identity") +
  ggtitle("Original distribution") +
  theme_classic()
h2 <- ggplot(value_imputed, aes(x = imputed_zero)) +
  geom_histogram(fill = "#15ad4f", color = "#000000", position = "identity") +
  ggtitle("Constant-imputed distribution of $200,000") +
  theme_classic()
h3 <- ggplot(value_imputed, aes(x = imputed_mean)) +
  geom_histogram(fill = "#1543ad", color = "#000000", position = "identity") +
  ggtitle("Mean-imputed distribution") +
  theme_classic()
h4 <- ggplot(value_imputed, aes(x = imputed_median)) +
  geom_histogram(fill = "#ad8415", color = "#000000", position = "identity") +
  ggtitle("Median-imputed distribution") +
  theme_classic()

plot_grid(h1, h2, h3, h4, nrow = 2, ncol = 2)

```

\newpage

**MICE** stands for Multivariate Imputation via Chained Equations. It assumes the missing values are **missing at random** (MAR), which we explained previously.  

A survey analyst can use the following MICE imputation methods from the MICE package:

* pmm: Predictive mean matching
* cart: Classification and regression trees
* laso.norm: Lasso linear regression.

```{r miceMethods, cache=TRUE, message=FALSE, echo=FALSE}
mice_imputed <- data.frame(
  original = imp$Total,
  imputed_pmm = complete(mice(imp, method = "pmm"))$Total,
  imputed_cart = complete(mice(imp, method = "cart"))$Total,
  imputed_lasso = complete(mice(imp, method = "lasso.norm"))$Total
)

ImputedValues <- subset(mice_imputed, is.na(original))

h5 <- ggplot(mice_imputed, aes(x = original)) +
  geom_histogram(fill = "#ad1538", color = "#000000", position = "identity") +
  ggtitle("Original distribution") +
  theme_classic()
h6 <- ggplot(mice_imputed, aes(x = imputed_pmm)) +
  geom_histogram(fill = "#ad8415", color = "#000000", position = "identity") +
  ggtitle("Imputed by PMM") +
  theme_classic()
h7 <- ggplot(mice_imputed, aes(x = imputed_cart)) +
  geom_histogram(fill = "#15ad4f", color = "#000000", position = "identity") +
  ggtitle("Imputed by CART") +
  theme_classic()
h8 <- ggplot(mice_imputed, aes(x = imputed_lasso)) +
  geom_histogram(fill = "#1543ad", color = "#000000", position = "identity") +
  ggtitle("Imputed by Lasso") +
  theme_classic()

plot_grid(h5, h6, h7, h8, nrow = 2, ncol = 2)

# Only show the 23 NAs for Total

mice_imputedGat <- gather(filter(mice_imputed, is.na(mice_imputed$original)), key = "Method", value = "Imputed")

ggplot(mice_imputedGat, aes(x = Method, y = Imputed)) +
  geom_col(position = "jitter") +
  ggtitle("Comparison of Values Imputed by Three MICE Methods") +
  theme_classic()

```

\newpage

The Miss Forest imputation technique is based on the Random Forest algorithm. It’s a non-parametric imputation method, which means it makes no assumptions about the function form (the distribution of values for the variable), but instead tries to estimate the function in a way that’s closest to the data points.

In other words, it builds a random forest model for each variable and then uses the model to predict missing values.

```{r, missForest, eval=FALSE}
library(missForest)  # did so with dependencies = TRUE per PDF on CRAN

impNoNAs <- imp[  , 1:4] # removed Bonus and Total because they have NAs
impNoNAs <- impNoNAs[-c(116, 134), ] # missing values in Reports
impNoNAs <- impNoNAs[-18, ] # missing value in Reports

colnames(impNoNAs) <- c("Years", "Staff", "JDs", "Total")

# dput(impNoNAs) for Stack Overflow

# writexl::write_xlsx(impNoNAs, "C:/Users/rees/Documents/R/Projects/LAWYER Hornbooks/5Surveys/SavSurvBlog/imputationdf.xlsx")  # changed some variable names

# summary(impNoNAs)
str(impNoNAs)

impNoNAs <- as.data.frame(impNoNAs)

impFor <- missForest(impNoNAs)

funModeling::v_compare(impNoNAs$Total, impFor$ximp$Total) #20 imputed Totals!

# missForest_imputed <- data.frame(
#   original = impNoNAs$Base,
#   imputed_missForest = missForest(impNoNAs)$ximp$Base
# )

# Warning: argument is not numeric or logical: returning NAWarning: argument is not numeric or logical: returning NAWarning: argument is not numeric or logical: returning NAWarning: argument is not numeric or logical: returning NAWarning: The response has five or fewer unique values.  Are you sure you want to do regression?Error in randomForest.default(x = obsX, y = obsY, ntree = ntree, mtry = mtry,  : 
#   length of response must be the same as predictors

h1 <- ggplot(impNoNAs, aes(x = original)) +
  geom_histogram(fill = "#ad1538", color = "#000000", position = "identity") +
  ggtitle("Original distribution") +
  theme_classic()

h2 <- ggplot(missForest_imputed, aes(x = imputed_missForest)) +
  geom_histogram(fill = "#15ad4f", color = "#000000",, position = "identity") +
  ggtitle("Original distribution") +
  theme_classic()

plot_grid(h1, h2, h3, nrow = 1, ncol = 2)


```


# Heteroskedasticity (the shape of the distribution curve)

```{r ranges, eval=FALSE}
# olsrr provides the following 4 tests for detecting heteroscedasticity: Bartlett Test; Breusch Pagan Test; Score Test;   F Test
# %http://www.brodrigues.co/blog/2018-07-08-rob_stderr/  good ideas with tidyr: rename and heteroskedasticity. 

library(olsrr)

ols_test_bartlett(data = imp$Total, 'Reports', 'Lawyers')

```

# Variations on dataviz

The person who prepares the plots of a survey report has a wealth of choices.  To give a peek at that forest, here are nine trees.  Each one represents graphically the same made-up data: how many COOs were there in a survey by three titles: Chief Operating Officer, Executive Director, and Firm Administrator.

```{r plotTypes}
df <- data.frame(Title = c("COO", "Exec. Dir.", "Firm Administrator"),
                 Incumbents = c(40, 20, 10))

df2 <- data.frame(Title = c(rep(1, 40), rep(2, 20), rep(3, 10)),
                  Incumbents = c(rep("COO", 40), rep("EDir", 20), rep("Admin'or", 10)))

# bar
p1 <- ggplot(df, aes(x = Title, y = Incumbents)) +
  geom_bar(fill = "#ad1538", color = "#000000", stat = "identity") +
  ggtitle("Column Plot") +
  theme_classic() 

# pie; windrows, coxcomb
p2 <- ggplot(df, aes(x = Title, y = Incumbents, )) +
  geom_bar(color = "#ad1538", stat = "identity") +
  coord_polar(theta = "x") +
  ggtitle("Pie/Polar Plot") +
  theme_classic() 

# scatter
p3 <- ggplot(df2, aes(x = Incumbents, y = Title, fill = Title)) +
  geom_jitter(pch = 20, size = 6) +
  ggtitle("Scatter Plot") +
  theme_classic() +
  theme(legend.position = "none")

# parliament  https://r-charts.com/part-whole/ggparliament/
library(ggparliament) # from clients/Old Client Directories/ChildLA/covid19/covidBook.Rmd

geo_semicircle <- parliament_data(election_data = df, 
                                  type = "semicircle", # Parliament type 
                                  parl_rows = 3, 
                                  party_seats = df$Incumbents)

p4 <- ggplot(geo_semicircle, aes(x = x, y = y, colour = Title)) +
  geom_parliament_seats() +
  labs(colour = NULL, title = "Parliament Plot of Titles") +
  theme_ggparliament()

  # scale_colour_colorblind()
```


```{r plotTypesMore}
# stacked (segment?)
df2$Type <- rep("Type", 70)

p5 <- ggplot(df2, aes(x = Type, y = Incumbents, fill = Title)) +
  geom_bar(stat = "identity", position = "stack") +
  labs(x = NULL, y = "Incumbents", title = "Stacked Bar Plot") +
  theme_classic() +
  guides(fill = FALSE) +
  coord_flip()

# see page 5 of Axiom In-House

# line
p6 <- ggplot(df, aes(x = Title, y = Incumbents, group = 1)) +
  geom_line() +
  ggtitle("Line Plot") +
  theme_classic() 

# lollipop
p7 <- ggplot(df, aes(y = Incumbents, fill = Title)) +
  geom_bar(stat = "count", position = "stack") +
  ggtitle("Lollipop Plot") +
  theme_classic() +
  coord_flip()

# segment  see example on ACC page 38
p8 <- ggplot(df, aes(x = Incumbents, color = Title)) +
  geom_segment(stat = "identity", aes(x = 0, xend = df$Incumbents[1], y = 5, yend = 5),
                                      linetype = "solid", linewidth = 8, lineend = "round")

ggplot(df, aes(x = Incumbents, color = Title)) +
  geom_segment(stat = "identity",
               aes(x = c(0, 40, 60),
                   xend = c(Incumbents[1], Incumbents[2] + Incumbents[1], Incumbents[3] + Incumbents[2] + Incumbents[3]),
                   y = 5, yend = 5),
               linetype = "solid", linewidth = 8, lineend = "round")
```


```{r plotTypesEvenMore}
# dotplot
p9 <- ggplot(df2) +
  geom_dotplot(method = "histodot", aes(x = Incumbents, fill = Title), position = position_jitter(height = 0.1, width = 0.1), size = 0.2, binwidth = 0.5) +  
  labs(x = "Titles", y = "Incumbents", title = "Count of Titles") +
  theme_classic() +
  # scale_fill_manual(values = colorRampPalette(brewer.pal(12, "Accent"))(colourCount)) +
  scale_y_continuous(breaks = NULL) +
  theme(axis.title.y =  element_blank()) +
  theme(legend.position = "bottom") +
  guides(fill=guide_legend(nrow=2)) +
  theme(legend.title=element_blank())

# bubble plot by area, like the ACC CLO chart on page 27

# donut plot doughnut

# See page 4 or Relativity Ari Kaplan report, with six donuts
# see page 4 of Axiom In-house for 3 and 5 slice donuts
```


```{r plotTypesCowPlot}
# 
# See dot plot
# https://towardsdatascience.com/unimpressed-with-your-scatter-and-bar-plots-give-these-four-classic-alternatives-a-try-a3bab20d4872

# filled outter circle with percentages inside
# See Axiom in-house page 6

cowplot::plot_grid(p1, p2, p3, p4, p5, p6, p7, p8, p9,
                   labels = c('', '', '', '', '', '', '', '', ''),
                   nrow = 3)

```

a so-called "parliament plot" (aka "horseshoe" plot). The Country posts are black and account for almost exactly half of the collected posts (53%). International posts are brownish and make up roughly one-third of the total (35%). Simulation and None complete the parliament seats in the lower right.


# Cluster the COOs

Hierarchical DBSCAN (HDBSCAN)

Density-based spatial clustering of applications with noise (DBSCAN) is a data clustering algorithm. It is a density-based clustering non-parametric algorithm: given a set of points in some space, it groups together points that are closely packed together (points with many nearby neighbors), marking as outliers points that lie alone in low-density regions (whose nearest neighbors are too far away). 

We can take the data from over 200 law firm Chief Operating Officers and see whether the hierarchical version of DBSCAN produces useful insights.

```{r dbscan, eval=FALSE}
# https://cran.r-project.org/web/packages/dbscan/vignettes/hdbscan.html
# https://medium.com/@dhavalmainkar01/understanding-clustering-6c156b0289ef  Python

library(dbscan)
# data("moons") # used in vignettes, with only two variables

cl <- hdbscan(filter(imp, Total > 1), minPts = 10) 
# Error in kNN(x, k = k, sort = TRUE) : x has to be a numeric matrix.
# removed NAs of Total
# I picked minPts at random

cl

plot(filter(imp, Total > 1), col=cl$cluster+1, pch=20)

plot(cl$hc, main="HDBSCAN* Hierarchy")

plot(cl, show_flat = TRUE)

```

The HDBSCAN* hierarchy is useful, but for larger datasets it can become overly cumbersome since every data point is represented as a leaf somewhere in the hierarchy. The hdbscan object comes with a powerful visualization tool that plots the ‘simplified’ hierarchy(see [2] for more details), which shows cluster-wide changes over an infinite number of eps thresholds. It is the default visualization dispatched by the ‘plot’ method


<!--chapter:end:ImputationPractice.Rmd-->

